<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ramin&#39;s Homepage</title>
    <link>https://mrnabati.github.io/</link>
      <atom:link href="https://mrnabati.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Ramin&#39;s Homepage</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2020 Ramin Nabati</copyright><lastBuildDate>Sun, 15 Nov 2020 11:01:13 -0500</lastBuildDate>
    <image>
      <url>https://mrnabati.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Ramin&#39;s Homepage</title>
      <link>https://mrnabati.github.io/</link>
    </image>
    
    <item>
      <title>CenterFusion</title>
      <link>https://mrnabati.github.io/projects/centerfusion/</link>
      <pubDate>Sun, 15 Nov 2020 11:01:13 -0500</pubDate>
      <guid>https://mrnabati.github.io/projects/centerfusion/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The perception system in autonomous vehicles is responsible for detecting and
tracking the surrounding objects. This is usually done by taking advantage of
several sensing modalities to increase robustness and accuracy, which makes sensor
fusion a crucial part of the perception system. In this paper, we focus on the
problem of radar and camera sensor fusion and propose a middle-fusion approach
to exploit both radar and camera data for 3D object detection. Our approach,
called CenterFusion, first uses a center point detection network to detect objects
by identifying their center points on the image. It then solves the key data
association problem using a novel frustum-based method to associate the radar
detections to their corresponding object’s center point. The associated radar
detections are used to generate radar-based feature maps to complement the image
features, and regress to object properties such as depth, rotation and velocity.&lt;/p&gt;
&lt;h2 id=&#34;our-approach&#34;&gt;Our Approach&lt;/h2&gt;
&lt;p&gt;We propose CenterFusion, a middle-fusion approach to exploit radar and camera
data for 3D object detection. CenterFusion focuses on associating radar
detections to preliminary detection results obtained from the image, then generates
radar feature maps and uses it in addition to image features to accurately estimate
3D bounding boxes for objects. Particularly, we generate preliminary 3D detections
using a key point detection network, and propose a novel frustum-based radar
association method to accurately associate radar detections to their corresponding
objects in the 3D space. These radar detections are then mapped to the
image plane and used to create feature maps to complement
the image-based features. Finally, the fused features are
used to accurately estimate objects’ 3D properties such as
depth, rotation and velocity. The network architecture for
CenterFusion is shown in the figure below.&lt;/p&gt;


















&lt;figure id=&#34;figure-centerfusion-network-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/architecture.jpg&#34; data-caption=&#34;CenterFusion network architecture&#34;&gt;


  &lt;img src=&#34;./images/architecture.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    CenterFusion network architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;center-point-detection&#34;&gt;Center Point Detection&lt;/h3&gt;
&lt;p&gt;We adopt the &lt;a href=&#34;https://arxiv.org/abs/1904.07850&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CenterNet&lt;/a&gt; detection network for generating preliminary detections on the image. The image features are first extracted using a fully convolutional encoder-decoder backbone network. We follow &lt;a href=&#34;https://arxiv.org/abs/1904.07850&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CenterNet&lt;/a&gt;
and use a modified version of the &lt;a href=&#34;https://arxiv.org/abs/1707.06484&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Layer Aggregation&lt;/a&gt;
(DLA) network as the backbone. The extracted image
features are then used to predict object center points on the
image, as well as the object 2D size (width and height), center offset, 3D dimensions, depth and rotation. These values
are predicted by the primary regression heads as shown in
the network architecture figure. Each primary regression head consists of a 3×3
convolution layer with 256 channels and a 1×1 convolutional layer to generate the desired output. This provides
an accurate 2D bounding box as well as a preliminary 3D
bounding box for every detected object in the scene.&lt;/p&gt;
&lt;h3 id=&#34;radar-association&#34;&gt;Radar Association&lt;/h3&gt;
&lt;p&gt;The center point detection network only uses the image
features at the center of each object to regress to all other
object properties. To fully exploit radar data in this process, we first need to associate the radar detections to their
corresponding object on the image plane. To accomplish
this, a naive approach would be mapping each radar detection point to the image plane and associating it to an object
if the point is mapped inside the 2D bounding box of that
object. This is not a very robust solution, as there is not a
one-to-one mapping between radar detections and objects
in the image; Many objects in the scene generate multiple
radar detections, and there are also radar detections that do
not correspond to any object. Additionally, because the z
dimension of the radar detection is not accurate (or does
not exist at all), the mapped radar detection might end up
outside the 2D bounding box of its corresponding object.
Finally, radar detections obtained from occluded objects
would map to the same general area in the image, which
makes differentiating them in the 2D image plane difficult,
if possible at all.&lt;/p&gt;
&lt;p&gt;We develop a frustum association method that uses the object’s 2D bounding
box as well as its estimated depth and size to create a 3D
Region of Interest (RoI) frustum for the object. Having an
accurate 2D bounding box for an object, we create a frustum for that object as shown in the figure below. This significantly
narrows down the radar detections that need to be checked
for association, as any point outside this frustum can be ignored. We then use the
estimated object depth, dimension
and rotation to create a RoI around the object, to further
filter out radar detections that are not associated with this
object. If there are multiple radar detections inside this RoI,
we take the closest point as the radar detection corresponding to this object.&lt;/p&gt;


















&lt;figure id=&#34;figure-frustum-association-an-object-detected-using-the-image-features-left-generating-the-roi-frustum-based-on-objects-3d-bounding-box-middle-and-the-bev-of-the-roi-frustum-showing-radar-detections-inside-the-frustum-right-delta-is-used-to-increase-the-frustum-size-in-the-testing-phase-hatd-is-the-ground-truth-depth-in-the-training-phase-and-the-estimated-object-depth-in-the-testing-phase&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/frustum.jpg&#34; data-caption=&#34;Frustum association. An object detected using the image features (left), generating the ROI frustum based on object&amp;amp;rsquo;s 3D bounding box (middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right). $\delta$ is used to increase the frustum size in the testing phase. $\hat{d}$ is the ground truth depth in the training phase and the estimated object depth in the testing phase.&#34;&gt;


  &lt;img src=&#34;./images/frustum.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Frustum association. An object detected using the image features (left), generating the ROI frustum based on object&amp;rsquo;s 3D bounding box (middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right). $\delta$ is used to increase the frustum size in the testing phase. $\hat{d}$ is the ground truth depth in the training phase and the estimated object depth in the testing phase.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The RoI frustum approach makes associating overlapping objects effortless, as objects are separated in the 3D
space and would have separate RoI frustums. It also eliminates the multi-detection association problem, as only the
closest radar detection inside the RoI frustum is associated
to the object. It does not, however, help with the inaccurate
z dimension problem, as radar detections might be outside
the ROI frustum of their corresponding object due to their
inaccurate height information.&lt;/p&gt;
&lt;p&gt;To address the inaccurate height information problem, we introduce a radar point cloud preprocessing step called pillar expansion, where each radar
point is expanded to a fixed-size pillar, as illustrated in the figure below. Pillars create a better representation for the physical objects detected by the radar, as these detections are now associated with a dimension in the 3D space. Having this new
representation, we simply consider a radar detection to be
inside a frustum if all or part of its corresponding pillar is
inside the frustum, as illustrated in the network architecture figure.&lt;/p&gt;


















&lt;figure id=&#34;figure-expanding-radar-points-to-3d-pillars-top-image-directly-mapping-the-pillars-to-the-image-and-replacing-with-radar-depth-information-results-in-poor-association-with-objects-center-and-many-overlapping-depth-values-middle-image-frustum-association-accurately-maps-the-radar-detections-to-the-center-of-objects-and-minimizes-overlapping-bottom-image&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/pillars.jpg&#34; data-caption=&#34;Expanding radar points to 3D pillars (top image). Directly mapping the pillars to the image and replacing with radar depth information results in poor association with objects&amp;#39; center and many overlapping depth values (middle image). Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping (bottom image).&#34;&gt;


  &lt;img src=&#34;./images/pillars.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Expanding radar points to 3D pillars (top image). Directly mapping the pillars to the image and replacing with radar depth information results in poor association with objects&#39; center and many overlapping depth values (middle image). Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping (bottom image).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In the above figure, radar detections are only associated to objects with a valid ground truth or detection box, and only if all or part of the radar detection pillar is inside the box. Frustum association also prevents associating radar detections caused by background objects such as buildings to foreground objects, as seen in the case of pedestrians on the right hand side of the image.&lt;/p&gt;
&lt;h3 id=&#34;radar-feature-extraction&#34;&gt;Radar Feature Extraction&lt;/h3&gt;
&lt;p&gt;After associating radar detections to their corresponding
objects, we use the depth and velocity of the radar detections to create complementary features for the image. Particularly, for every radar detection associated to an object,
we generate three heat map channels centered at and inside
the object’s 2D bounding box, as shown in the figure above. The
width and height of the heatmaps are proportional to the
object&amp;rsquo;s 2D bounding box, and are controlled by a parameter $\alpha$. The heatmap values are the normalized object depth
$d$ and also the $x$ and $y$ components of the radial velocity
($v_x$ and $v_y$) in the egocentric coordinate system:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation*}
F_{x,y,i}^{j} = \frac{1}{M_i}
\begin{cases}
f_i &amp;amp; \hskip{5pt} |x-c_{x}^{j}|\leq \alpha w^j \hspace{5pt} \text{and}
\hspace{5pt} |y-c_{y}^{i}| \leq \alpha h^j \\&lt;br&gt;
0 &amp;amp; \hskip{5pt} \text{otherwise}
\end{cases}
\end{equation*}
$$&lt;/p&gt;
&lt;p&gt;where $i \in 1, 2, 3$ is the feature map channel, $M_i$
is a normalizing factor, $f_i$ is the feature value ($d$, $v_x$ or $v_y$),
$c^j_x$ and $c^j_y$ are the $x$ and $y$ coordinates of the $j$th object’s center
point on the image and $w^j$ and $h^j$ are the width and height of the
$j$th object’s 2D bounding box. If two objects have
overlapping heatmap areas, the one with a smaller depth
value dominates, as only the closest object is fully visible in
the image.&lt;/p&gt;
&lt;p&gt;The generated heat maps are then concatenated to the
image features as extra channels. These features are used
as inputs to the secondary regression heads to recalculate
the object’s depth and rotation, as well as velocity and attributes. The velocity regression head estimates the x and
y components of the object’s actual velocity in the vehicle
coordinate system. The attribute regression head estimates
different attributes for different object classes, such as moving or parked for the Car class and standing or sitting for the
Pedestrian class. The secondary regression heads consist of
three convolutional layers with 3$\times$3 kernels followed by
a 1$\times$1 convolutional layer to generate the desired output.
The extra convolutional layers compared to the primary regression heads help with
learning higher level features from
the radar feature maps. The last step is decoding the regression head results into
3D bounding boxes. The box decoder
block uses the estimated depth, velocity, rotation, and attributes from the secondary
regression heads, and takes the other object properties from the primary heads.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;We compare our radar and camera fusion network with
the state-of-the-art camera-based models on the nuScenes
benchmark, and also a LIDAR based method. Table 1
shows the results on both test and validation splits of the
nuScenes dataset.&lt;/p&gt;


















&lt;figure id=&#34;figure-quantitative-results&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/results_quant.png&#34; data-caption=&#34;Quantitative results&#34;&gt;


  &lt;img src=&#34;./images/results_quant.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Quantitative results
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The detection results compared to the ground truth in camera-view and birds eye view
are shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-qualitative-results-from-centerfusion-row-1--2-and-centernet-row-3--4-in-camera-view-and-bev-in-the-bev-plots-detection-boxes-are-shown-in-cyan-and-ground-truth-boxes-in-red-the-radar-point-cloud-is-shown-in-green-red-and-blue-arrows-on-objects-show-the-ground-truth-and-predicted-velocity-vectors-respectively&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/results_qual.png&#34; data-caption=&#34;Qualitative results from CenterFusion (row 1 &amp;amp;amp; 2) and CenterNet (row 3 &amp;amp;amp; 4) in camera view and BEV. In the BEV plots, detection boxes are shown in cyan and ground truth boxes in red. The radar point cloud is shown in green. Red and blue arrows on objects show the ground truth and predicted velocity vectors respectively.&#34;&gt;


  &lt;img src=&#34;./images/results_qual.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Qualitative results from CenterFusion (row 1 &amp;amp; 2) and CenterNet (row 3 &amp;amp; 4) in camera view and BEV. In the BEV plots, detection boxes are shown in cyan and ground truth boxes in red. The radar point cloud is shown in green. Red and blue arrows on objects show the ground truth and predicted velocity vectors respectively.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;For a more detailed discussion on the results and also the ablation study, see our
&lt;a href=&#34;https://arxiv.org/abs/2011.04841&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WACV 2021 conference paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</title>
      <link>https://mrnabati.github.io/publication/03_centerfusion/</link>
      <pubDate>Thu, 12 Nov 2020 23:29:22 -0500</pubDate>
      <guid>https://mrnabati.github.io/publication/03_centerfusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Radar-Camera Sensor Fusion and Depth Estimation</title>
      <link>https://mrnabati.github.io/projects/radar-camera-sensor-fusion/</link>
      <pubDate>Wed, 14 Oct 2020 21:08:24 -0500</pubDate>
      <guid>https://mrnabati.github.io/projects/radar-camera-sensor-fusion/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this project, we designed and implemented a radar-camera fusion algorithm for joint object detection and distance estimation in
autonomous driving applications. The proposed method is
designed as a two-stage object detection network that fuses
radar point clouds and learned image features to generate
accurate object proposals. For every object proposal, a depth
value is also calculated to estimate the object’s distance from
the vehicle. These proposals are then fed into the second
stage of the detection network for object classification. We
evaluate our network on the &lt;a href=&#34;https://www.nuscenes.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nuScenes dataset&lt;/a&gt;, which
provides synchronized data from multiple radar and camera
sensors on a vehicle. Our experiments show that the proposed
method outperforms other radar-camera fusion methods in
the object detection task and is capable of accurately estimating distance for all detected objects.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;


















&lt;figure id=&#34;figure-network-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/architecture.jpg&#34; data-caption=&#34;Network architecture&#34;&gt;


  &lt;img src=&#34;./images/architecture.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Network architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Our proposed sensor fusion network is shown in the figure above.
The network takes radar point clouds and RGB images as
input and generates accurate object proposals for a two-stage
object detection framework. We take a middle-fusion approach for fusing the radar and image data, where outputs of each sensor are processed independently first, and are merged
at a later stage for more processing. More specifically, we
first use the radar detections to generate 3D object proposals,
then map the proposals to the image and use the image
features extracted by a backbone network to improve their
localization. These proposals are then merged with image-based proposals generated in a RPN, and are fed to the
second stage for classification. All generated proposals are
associated with an estimated depth, calculated either directly
from the radar detections, or via a distance regressor layer
in the RPN network.&lt;/p&gt;
&lt;h3 id=&#34;proposal-generation&#34;&gt;Proposal Generation&lt;/h3&gt;
&lt;p&gt;We treat every radar point as a
stand-alone detection and generate 3D object proposals for
them directly without any feature extraction. These proposals
are generated using predefined 3D anchors for every object
class in the dataset. Each 3D anchor is parameterized as
$(x, y, z, w, l, h, r)$, where $(x, y, z)$ is the center, $(w, l, h)$ is
the size, and $r$ is the orientation of the box in vehicle’s
coordinate system. The anchor size, $(w, l, h)$, is fixed for
each object category, and is set to the average size of the
objects in each category in the training dataset. For every radar point, we generate $2n$
boxes from the 3D anchors, where $n$ is the number of object
classes in the dataset, each having two different orientations at $0 $ and $90$ degrees. The 3D anchors for a radar detection is shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-3d-anchors-for-one-radar-detection-point&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/3d_prop.jpg&#34; data-caption=&#34;3D anchors for one radar detection point&#34;&gt;


  &lt;img src=&#34;./images/3d_prop.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    3D anchors for one radar detection point
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In the next step, all 3D anchors are mapped to the image
plane and converted to equivalent 2D bounding boxes by
finding the smallest enclosing box for each mapped anchor. Since every 3D proposal is generated from a radar detection,
it has an accurate distance associated with it. This distance is
used as the proposed distance for the generated 2D bounding
box. This is illustrated in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-3d-anchors-for-one-radar-detection-point&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/2d_prop.jpg&#34; data-caption=&#34;3D anchors for one radar detection point&#34;&gt;


  &lt;img src=&#34;./images/2d_prop.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    3D anchors for one radar detection point
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;All generated 2D proposals are fed into the
Radar Proposal Refinement (RPR) subnetwork. This is where
the information obtained from the radars (radar proposals) is
fused with the information obtained from the camera (image
features). RPR uses the features extracted from the image
by the backbone network to adjust the size and location
of the radar proposals on the image. As radar detections
are not always centered on the corresponding objects on
the image, the generated 3D anchors and corresponding 2D
proposals might be offset as well. The box regressor layer in
the RPR uses the image features inside each radar proposal
to regress offset values for the proposal corner points. The
RPR also contains a box classification layer, which estimates
an objectness score for every radar proposal. The objectness
score is used to eliminate proposals that are generated by
radar detections coming from background objects, such as
buildings and light poles. Figures below show the resulting 2D radar
proposals before and after the refinement step.&lt;/p&gt;


















&lt;figure id=&#34;figure-radar-proposals-before-refinement&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/2d_all_before.jpg&#34; data-caption=&#34;Radar proposals before refinement&#34;&gt;


  &lt;img src=&#34;./images/2d_all_before.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Radar proposals before refinement
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-radar-proposals-after-refinement&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/2d_all_after.jpg&#34; data-caption=&#34;Radar proposals after refinement&#34;&gt;


  &lt;img src=&#34;./images/2d_all_after.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Radar proposals after refinement
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The Radar proposals are then merged with image-based proposals obtained from a Region Proposal Network (RPN). Before using these proposals in the next
stage, redundant proposals are removed by applying NonMaximum Suppression (NMS). The NMS would normally
remove overlapping proposals without discriminating based
on the bounding box’s origin, but we note that radar-based
proposals have more reliable distance information than the
image-based proposals. This is because image-based distances are estimated only from 2D image feature maps with
no depth information. To make sure the radar-based distances
are not unnecessarily discarded in the NMS process, we
first calculate the Intersection over Union (IoU) between
radar and image proposals. Next we use an IoU threshold
to find the matching proposals, and overwrite the imagebased distances by their radar-based counterparts for these
matching proposals.&lt;/p&gt;
&lt;h3 id=&#34;detection-network&#34;&gt;Detection Network&lt;/h3&gt;
&lt;p&gt;The inputs to the second stage detection network are
the feature map from the image and object proposals. The
structure of this network is similar to &lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast R-CNN&lt;/a&gt;. The
feature map is cropped for every object proposals and is fed
into the RoI pooling layer to obtain feature vectors of the
same size for all proposals. These feature vectors are further
processed by a set of fully connected layers and are passed to
the softmax and bounding box regression layers. The output
is the category classification and bounding box regression
for each proposal, in addition to the distance associated to
every detected object. Similar to the RPN network, we use
a cross entropy loss for object classification and a Smooth
L1 loss for the box regression layer.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The performance of our method is shown in the table below. This
table shows the overall Average Precision (AP) and Average
Recall (AR) for the detection task, and Mean Absolute
Error for the distance estimation task. We use the &lt;a href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faster RCNN&lt;/a&gt; network as our image-based detection baseline, and
compare our results with &lt;a href=&#34;https://arxiv.org/pdf/1905.00526.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RRPN&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;AP&lt;/th&gt;
&lt;th&gt;AP50&lt;/th&gt;
&lt;th&gt;AP75&lt;/th&gt;
&lt;th&gt;AR&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Faster R-CNN&lt;/td&gt;
&lt;td&gt;34.95&lt;/td&gt;
&lt;td&gt;58.23&lt;/td&gt;
&lt;td&gt;36.89&lt;/td&gt;
&lt;td&gt;40.21&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RRPN&lt;/td&gt;
&lt;td&gt;35.45&lt;/td&gt;
&lt;td&gt;59.00&lt;/td&gt;
&lt;td&gt;37.00&lt;/td&gt;
&lt;td&gt;42.10&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ours&lt;/td&gt;
&lt;td&gt;35.60&lt;/td&gt;
&lt;td&gt;60.53&lt;/td&gt;
&lt;td&gt;37.38&lt;/td&gt;
&lt;td&gt;42.10&lt;/td&gt;
&lt;td&gt;2.65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The per-class performance is show in the table below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Car&lt;/th&gt;
&lt;th&gt;Truck&lt;/th&gt;
&lt;th&gt;Person&lt;/th&gt;
&lt;th&gt;Bus&lt;/th&gt;
&lt;th&gt;Bicycle&lt;/th&gt;
&lt;th&gt;Motorcycle&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Faster R-CNN&lt;/td&gt;
&lt;td&gt;51.46&lt;/td&gt;
&lt;td&gt;33.26&lt;/td&gt;
&lt;td&gt;27.06&lt;/td&gt;
&lt;td&gt;47.73&lt;/td&gt;
&lt;td&gt;24.27&lt;/td&gt;
&lt;td&gt;25.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RRPN&lt;/td&gt;
&lt;td&gt;41.80&lt;/td&gt;
&lt;td&gt;44.70&lt;/td&gt;
&lt;td&gt;17.10&lt;/td&gt;
&lt;td&gt;57.20&lt;/td&gt;
&lt;td&gt;21.40&lt;/td&gt;
&lt;td&gt;30.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ours&lt;/td&gt;
&lt;td&gt;52.31&lt;/td&gt;
&lt;td&gt;34.45&lt;/td&gt;
&lt;td&gt;27.59&lt;/td&gt;
&lt;td&gt;48.30&lt;/td&gt;
&lt;td&gt;25.00&lt;/td&gt;
&lt;td&gt;25.97&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Figures below show the detection results for two different scenes:&lt;/p&gt;


















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/res_1.jpg&#34; &gt;


  &lt;img src=&#34;./images/res_1.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;



















&lt;figure id=&#34;figure-detection-results&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/res_2.jpg&#34; data-caption=&#34;Detection results&#34;&gt;


  &lt;img src=&#34;./images/res_2.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Detection results
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Adv. PyTorch: Configuring MS Visual Studio for Using PyToch C&#43;&#43; API in Windows</title>
      <link>https://mrnabati.github.io/2020/06/adv.-pytorch-configuring-ms-visual-studio-for-using-pytoch-c-api-in-windows/</link>
      <pubDate>Mon, 22 Jun 2020 20:17:31 -0400</pubDate>
      <guid>https://mrnabati.github.io/2020/06/adv.-pytorch-configuring-ms-visual-studio-for-using-pytoch-c-api-in-windows/</guid>
      <description>&lt;p&gt;This tutorial will walk you through the required steps to configure
and use the PyTorch C++ API (LibTorch) in Microsoft Visual Studio. Although
the recommended build system for LibTorch is CMake, you might find yourself in
situations where you need to integrate your code into an existing Visual Studio
Project/Solution and don&amp;rsquo;t want to deal with CMake files in Windows. Following
the steps in this tutorial should get you up and running with LibTorch in
Visual Studio without needing to use CMake to build it. These steps
have been tested on Visual Studio 2019 and the CPU version of LibTorch 1.5.1.
Let&amp;rsquo;s get started!&lt;/p&gt;
&lt;h2 id=&#34;step-1-download-libtorch&#34;&gt;Step 1: Download LibTorch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Download and extract the CPU version of LibTorch for Windows from &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
Note that the &lt;strong&gt;Release&lt;/strong&gt; and &lt;strong&gt;Debug&lt;/strong&gt; versions have different download links,
so get the one you need depending on your target configuration. We work with the
Debug version here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assets/013_1.png&#34; alt=&#34;img1&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2-set-windows-environment-variables&#34;&gt;Step 2: Set Windows Environment Variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We&amp;rsquo;re going to create some environment variables to make things easier. Use the
following commands in a Windows Terminal to create an environment variable for
the LibTorch directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;setx LIBTORCH_DEBUG_HOME &amp;quot;C:\libtorch-debug-v1.5.1&amp;quot;
set LIBTORCH_DEBUG_HOME &amp;quot;C:\libtorch-debug-v1.5.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;&amp;quot;C:\libtorch-debug-v1.5.1&amp;quot;&lt;/code&gt; is the path to the extracted LibTorch
directory on your computer. Note that the &lt;code&gt;setx&lt;/code&gt; command creates the variable
globally for Windows, and the &lt;code&gt;set&lt;/code&gt; command creates it just for the current
session.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you want to build PyTorch C++ extensions, you&amp;rsquo;ll need to add the Python
header files to your Visual Studio project. Use the following commands to
create environment variables for your Python path. Note that if Python
was installed as part of the Visual Studio, the Python directory
should be in &lt;code&gt;&amp;quot;C:\Program Files (x86)\Microsoft Visual Studio\Shared&amp;quot;&lt;/code&gt;. Otherwise
locate your Python installation directory and change the path accordingly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;setx PYTHON_HOME &amp;quot;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64&amp;quot;
set PYTHON_HOME &amp;quot;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-configure-your-visual-studio-project&#34;&gt;Step 3: Configure Your Visual Studio Project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Open Visual Studio, create a project and make sure the Platform is set to
&lt;strong&gt;x64&lt;/strong&gt;. Following &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_frontend.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;,
let&amp;rsquo;s create a simple C++ file called &lt;code&gt;dcgan.cpp&lt;/code&gt; with the following contents
to test our setup later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;torch\torch.h&amp;gt;
#include &amp;lt;iostream&amp;gt;

int main(){
    torch::Tensor tensor = torch::eye(3);
    std::cout &amp;lt;&amp;lt; tensor &amp;lt;&amp;lt; std::endl;
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;Project Properties&lt;/strong&gt; under &lt;code&gt;C/C++ -&amp;gt; General -&amp;gt; Additional Include Directories&lt;/code&gt; add
the path to LibTorch and Python include folders:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$(LIBTORCH_DEBUG_HOME)\include
$(PYTHON_HOME)\include
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;assets/013_2.PNG&#34; alt=&#34;img2&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;Project Properties&lt;/strong&gt; under &lt;code&gt;Linker -&amp;gt; General -&amp;gt; Additional Library Directories&lt;/code&gt; add
the path to LibTorch and Python lib folders:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; $(LIBTORCH_DEBUG_HOME)\lib
 $(PYTHON_HOME)\lib
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;assets/013_3.PNG&#34; alt=&#34;img2&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;Project Properties&lt;/strong&gt; under &lt;code&gt;Linker -&amp;gt; Input -&amp;gt; Additional Dependencies&lt;/code&gt;
add the following libraries:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; torch.lib
 torch_cpu.lib
 c10.lib
 python37.lib
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;assets/013_4.PNG&#34; alt=&#34;img2&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We also need to add a &lt;strong&gt;Post-Build Event&lt;/strong&gt; to copy all the LibTorch &lt;code&gt;dll&lt;/code&gt;
files to the target directory after every build. Without these files you&amp;rsquo;ll be
getting a runtime error when executing your program. In &lt;strong&gt;Project Properties&lt;/strong&gt; under
&lt;code&gt;Build Events -&amp;gt; Post-Build Event -&amp;gt; Command Line&lt;/code&gt; add the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;copy /y $(LIBTORCH_DEBUG_HOME)\lib\*.dll $(TargetDir)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;assets/013_5.PNG&#34; alt=&#34;img2&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-4-build-and-run-the-code&#34;&gt;Step 4: Build and Run the Code&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Now you can go ahead and build the project. Make sure you choose the same
build configuration (Debug/Release) as the downloaded LibTorch package. To
test our setup, run the generated executable file. The output should be a
3x3 diagonal matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 1  0  0
 0  1  0
 0  0  1
[ CPUFloatType{3,3} ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Adv. PyTorch: Modifying the Last Layer</title>
      <link>https://mrnabati.github.io/2020/06/adv.-pytorch-modifying-the-last-layer/</link>
      <pubDate>Sun, 21 Jun 2020 16:42:11 -0400</pubDate>
      <guid>https://mrnabati.github.io/2020/06/adv.-pytorch-modifying-the-last-layer/</guid>
      <description>&lt;p&gt;All the pre-trained models provided in the &lt;code&gt;torchvision&lt;/code&gt; package in PyTorch are
trained on the &lt;a href=&#34;http://www.image-net.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ImageNet&lt;/a&gt; dataset and can be used out
of the box on this dataset. But often times you want to use these models on
other available image datasets or even your own custom dataset. This usually
requires modifying and fine-tuning the model to work with the new dataset.
Changing the output dimension of the last layer in the model is usually among
the first changes you need to make, and that&amp;rsquo;s the focus of this post.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with loading a pre-trained model from the &lt;code&gt;torchvision&lt;/code&gt; package. We
use the &lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VGG16&lt;/a&gt; model, pretrained on the
ImageNet dataset with 1000 object categories. Let&amp;rsquo;s take a look at the modules
on this model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torchvision.models as models

vgg16 = models.vgg16(pretrained=True)
print(vgg16._modules.keys())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;odict_keys([&#39;features&#39;, &#39;avgpool&#39;, &#39;classifier&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are only interested in the last layer, so let&amp;rsquo;s print the layers in the
&amp;lsquo;classifier&amp;rsquo; module:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(vgg16._modules[&#39;classifier&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=25088, out_features=4096, bias=True)
  (1): ReLU(inplace=True)
  (2): Dropout(p=0.5, inplace=False)
  (3): Linear(in_features=4096, out_features=4096, bias=True)
  (4): ReLU(inplace=True)
  (5): Dropout(p=0.5, inplace=False)
  (6): Linear(in_features=4096, out_features=1000, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the output dimension for the last layer is 1000. Let&amp;rsquo;s assume we
are going to use this model on the &lt;a href=&#34;http://cocodataset.org/#home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COCO dataset&lt;/a&gt;
with 80 object categories. To change the output dimension of the model to 80,
we simply replace the last sub-layer with a new Linear layer. The Linear layer
takes two required arguments: &lt;code&gt;in_features&lt;/code&gt; and &lt;code&gt;out_features&lt;/code&gt;. The &lt;code&gt;in_features&lt;/code&gt;
is going to be the same as before, and &lt;code&gt;out_features&lt;/code&gt; is goint to be 80:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;in_features = vgg16._modules[&#39;classifier&#39;][-1].in_features
out_features = 80
vgg16._modules[&#39;classifier&#39;][-1] = nn.Linear(in_features, out_features, bias=True)
print(vgg16._modules[&#39;classifier&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=25088, out_features=4096, bias=True)
  (1): ReLU(inplace=True)
  (2): Dropout(p=0.5, inplace=False)
  (3): Linear(in_features=4096, out_features=4096, bias=True)
  (4): ReLU(inplace=True)
  (5): Dropout(p=0.5, inplace=False)
  (6): Linear(in_features=4096, out_features=80, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s it! The output dimension is now 80. You need to keep in mind that by
replacing the last layer we removed any learned parameter in this layer. You
need to finetune the model on the new dataset at this point to learn the
parameters again.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adv. PyTorch: Freezing Layers</title>
      <link>https://mrnabati.github.io/2020/05/adv.-pytorch-freezing-layers/</link>
      <pubDate>Fri, 22 May 2020 13:42:11 -0400</pubDate>
      <guid>https://mrnabati.github.io/2020/05/adv.-pytorch-freezing-layers/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re planning to fine-tune a trained model on a different dataset, chances
are you&amp;rsquo;re going to freeze some of the early layers and only update the later
layers. I won&amp;rsquo;t go into the details of why you may want to freeze some layers
and which ones should be frozen, but I&amp;rsquo;ll show you how to do it in PyTorch.
Let&amp;rsquo;s get started!&lt;/p&gt;
&lt;p&gt;We first need a pre-trained model to start with. The
&lt;a href=&#34;https://pytorch.org/docs/stable/torchvision/models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;models subpackage&lt;/a&gt; in
the &lt;code&gt;torchvision&lt;/code&gt; package provides definitions for many of the poplular model
architectures for image classification. You can construct these models by simply
calling their constructor, which would initialize the model with random weights.
To use the pre-trained models from the PyTorch Model Zoo, you can call the
constructor with the &lt;code&gt;pretrained=True&lt;/code&gt; argument. Let&amp;rsquo;s load the pretrained
VGG16 model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torchvision.models as models

vgg16 = models.vgg16(pretrained=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start downloading the pretrained model into your computer&amp;rsquo;s PyTorch
cache folder, which usually is the &lt;code&gt;.cache/torch/checkpoints&lt;/code&gt; folder under your
home directory.&lt;/p&gt;
&lt;p&gt;There are multiple ways you can look into the model to see its modules and
layers. One way is using the &lt;code&gt;.modules()&lt;/code&gt; member function, which returns in
iterator containing all the member objects of the model. The &lt;code&gt;.modules()&lt;/code&gt;
functions recursively goes thruogh all the modules and submodules of the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(list(vgg16.modules()))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
), Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace=True)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace=True)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace=True)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace=True)
  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (18): ReLU(inplace=True)
  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace=True)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace=True)
  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (25): ReLU(inplace=True)
  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (27): ReLU(inplace=True)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace=True)
  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
), Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AdaptiveAvgPool2d(output_size=(7, 7)), Sequential(
  (0): Linear(in_features=25088, out_features=4096, bias=True)
  (1): ReLU(inplace=True)
  (2): Dropout(p=0.5, inplace=False)
  (3): Linear(in_features=4096, out_features=4096, bias=True)
  (4): ReLU(inplace=True)
  (5): Dropout(p=0.5, inplace=False)
  (6): Linear(in_features=4096, out_features=1000, bias=True)
), Linear(in_features=25088, out_features=4096, bias=True), ReLU(inplace=True), Dropout(p=0.5, inplace=False), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace=True), Dropout(p=0.5, inplace=False), Linear(in_features=4096, out_features=1000, bias=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s a lot of information spewed out onto the screen! Let&amp;rsquo;s use the
&lt;code&gt;.named_module()&lt;/code&gt; function instead, which returns a (name, module) tuple and
only print the names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for (name, module) in vgg16.named_modules():
    print(name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;features
features.0
features.1
features.2
features.3
features.4
features.5
features.6
features.7
features.8
features.9
features.10
features.11
features.12
features.13
features.14
features.15
features.16
features.17
features.18
features.19
features.20
features.21
features.22
features.23
features.24
features.25
features.26
features.27
features.28
features.29
features.30
avgpool
classifier
classifier.0
classifier.1
classifier.2
classifier.3
classifier.4
classifier.5
classifier.6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s much better! We can see the top level modules are &lt;em&gt;features&lt;/em&gt;, &lt;em&gt;avgpool&lt;/em&gt;
and &lt;em&gt;classifier&lt;/em&gt;. We can also see that the &lt;em&gt;features&lt;/em&gt; and &lt;em&gt;calssifier&lt;/em&gt; modules
consist of 31 and 7 layers respectively. These layers are not named, and only
have numbers associated with them. If you want to see an even more concise
representation of the network, you can use the &lt;code&gt;.named_children()&lt;/code&gt; function
which does not go inside the top level modules recursively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for (name, module) in vgg16.named_children():
    print(name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;features
avgpool
classifier
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s see what layers are there under the &lt;em&gt;features&lt;/em&gt; module. Here we use the
&lt;code&gt;.children()&lt;/code&gt; function to get the layers under the &lt;em&gt;features&lt;/em&gt; module, since
these layers are not &amp;lsquo;named&amp;rsquo;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for (name, module) in vgg16.named_children():
    if name == &#39;features&#39;:
        for layer in module.children():
            print(layer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can even go deeper and look at the parameters in each layer. Let&amp;rsquo;s get the
parameters of the first layer under the &lt;em&gt;features&lt;/em&gt; module:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for (name, module) in vgg16.named_children():
    if name == &#39;features&#39;:
        for layer in module.children():
            for param in layer.parameters():
                print(param)
            break
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Parameter containing:
tensor([[[[-5.5373e-01,  1.4270e-01,  5.2896e-01],
          [-5.8312e-01,  3.5655e-01,  7.6566e-01],
          [-6.9022e-01, -4.8019e-02,  4.8409e-01]],

         [[ 1.7548e-01,  9.8630e-03, -8.1413e-02],
          [ 4.4089e-02, -7.0323e-02, -2.6035e-01],
          [ 1.3239e-01, -1.7279e-01, -1.3226e-01]],

         [[ 3.1303e-01, -1.6591e-01, -4.2752e-01],
          [ 4.7519e-01, -8.2677e-02, -4.8700e-01],
          [ 6.3203e-01,  1.9308e-02, -2.7753e-01]]],


        [[[ 2.3254e-01,  1.2666e-01,  1.8605e-01],
          [-4.2805e-01, -2.4349e-01,  2.4628e-01],
          [-2.5066e-01,  1.4177e-01, -5.4864e-03]],

         [[-1.4076e-01, -2.1903e-01,  1.5041e-01],
          [-8.4127e-01, -3.5176e-01,  5.6398e-01],
          [-2.4194e-01,  5.1928e-01,  5.3915e-01]],

         [[-3.1432e-01, -3.7048e-01, -1.3094e-01],
          [-4.7144e-01, -1.5503e-01,  3.4589e-01],
          [ 5.4384e-02,  5.8683e-01,  4.9580e-01]]],


        [[[ 1.7715e-01,  5.2149e-01,  9.8740e-03],
          [-2.7185e-01, -7.1709e-01,  3.1292e-01],
          [-7.5753e-02, -2.2079e-01,  3.3455e-01]],

         [[ 3.0924e-01,  6.7071e-01,  2.0546e-02],
          [-4.6607e-01, -1.0697e+00,  3.3501e-01],
          [-8.0284e-02, -3.0522e-01,  5.4460e-01]],

         [[ 3.1572e-01,  4.2335e-01, -3.4976e-01],
          [ 8.6354e-02, -4.6457e-01,  1.1803e-02],
          [ 1.0483e-01, -1.4584e-01, -1.5765e-02]]],


        ...,


        [[[ 7.7599e-02,  1.2692e-01,  3.2305e-02],
          [ 2.2131e-01,  2.4681e-01, -4.6637e-02],
          [ 4.6407e-02,  2.8246e-02,  1.7528e-02]],

         [[-1.8327e-01, -6.7425e-02, -7.2120e-03],
          [-4.8855e-02,  7.0427e-03, -1.2883e-01],
          [-6.4601e-02, -6.4566e-02,  4.4235e-02]],

         [[-2.2547e-01, -1.1931e-01, -2.3425e-02],
          [-9.9171e-02, -1.5143e-02,  9.5385e-04],
          [-2.6137e-02,  1.3567e-03,  1.4282e-01]]],


        [[[ 1.6520e-02, -3.2225e-02, -3.8450e-03],
          [-6.8206e-02, -1.9445e-01, -1.4166e-01],
          [-6.9528e-02, -1.8340e-01, -1.7422e-01]],

         [[ 4.2781e-02, -6.7529e-02, -7.0309e-03],
          [ 1.1765e-02, -1.4958e-01, -1.2361e-01],
          [ 1.0205e-02, -1.0393e-01, -1.1742e-01]],

         [[ 1.2661e-01,  8.5046e-02,  1.3066e-01],
          [ 1.7585e-01,  1.1288e-01,  1.1937e-01],
          [ 1.4656e-01,  9.8892e-02,  1.0348e-01]]],


        [[[ 3.2176e-02, -1.0766e-01, -2.6388e-01],
          [ 2.7957e-01, -3.7416e-02, -2.5471e-01],
          [ 3.4872e-01,  3.0041e-02, -5.5898e-02]],

         [[ 2.5063e-01,  1.5543e-01, -1.7432e-01],
          [ 3.9255e-01,  3.2306e-02, -3.5191e-01],
          [ 1.9299e-01, -1.9898e-01, -2.9713e-01]],

         [[ 4.6032e-01,  4.3399e-01,  2.8352e-01],
          [ 1.6341e-01, -5.8165e-02, -1.9196e-01],
          [-1.9521e-01, -4.5630e-01, -4.2732e-01]]]], requires_grad=True)
Parameter containing:
tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,
         0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,
         0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,
         0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,
         0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,
         0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,
         0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,
         0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],
       requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have access to all the modules, layers and their parameters, we can
easily freeze them by setting the parameters&#39; &lt;code&gt;requires_grad&lt;/code&gt; flag to &lt;code&gt;False&lt;/code&gt;.
This would prevent calculating the gradients for these parameters in the
&lt;code&gt;backward&lt;/code&gt; step which in turn prevents the optimizer from updating them.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s freeze all the parameters in the &lt;em&gt;features&lt;/em&gt; module:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;layer_counter = 0
for (name, module) in vgg16.named_children():
    if name == &#39;features&#39;:
        for layer in module.children():
            for param in layer.parameters():
                param.requires_grad = False
            
            print(&#39;Layer &amp;quot;{}&amp;quot; in module &amp;quot;{}&amp;quot; was frozen!&#39;.format(layer_counter, name))
            layer_counter+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Layer &amp;quot;0&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;1&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;2&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;3&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;4&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;5&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;6&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;7&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;8&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;9&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;10&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;11&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;12&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;13&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;14&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;15&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;16&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;17&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;18&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;19&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;20&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;21&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;22&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;23&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;24&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;25&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;26&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;27&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;28&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;29&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
Layer &amp;quot;30&amp;quot; in module &amp;quot;features&amp;quot; was frozen!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that some of the parameters are frozen, the optimizer needs to be modified
to only get the parameters with &lt;code&gt;requires_grad=True&lt;/code&gt;. We can do this by writing
a Lambda function when constructing the optimizer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, vgg16.parameters()), lr=0.001)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now start training your partially frozen model!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Radar Region Proposal Network</title>
      <link>https://mrnabati.github.io/projects/rrpn/</link>
      <pubDate>Sun, 10 May 2020 17:25:44 -0400</pubDate>
      <guid>https://mrnabati.github.io/projects/rrpn/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Radar Region Proposal Network (RRPN) is a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object
proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for
each mapped Radar detection point. These anchor boxes are
then transformed and scaled based on the object’s distance
from the vehicle, to provide more accurate proposals for the
detected objects. The generated proposals can be used in any two-stage object detection network
such as Fast-RCNN. Relying only on Radar detections to generate object proposals makes an extremely fast RPN, making
it suitable for autonomous driving applications. Aside from
being a RPN for an object detection algorithm, the proposed
network also inherently acts as a sensor fusion algorithm by
fusing the Radar and camera data to obtain higher accuracy
and reliability.&lt;/p&gt;
&lt;p&gt;RRPN also provides an attention mechanism to focus the
underlying computational resources on the more important
parts of the input data. While in other object detection applications the entire image may be of equal importance. In
an autonomous driving application more attention needs to be
given to objects on the road. For example in a highway driving scenario, the perception system needs to be able to detect
all the vehicles on the road, but there is no need to dedicate resources to detect a picture of a vehicle on a billboard. A Radar
based RPN focuses only on the physical objects surrounding
the vehicle, hence inherently creating an attention mechanism
focusing on parts of the input image that are more important.&lt;/p&gt;
&lt;h2 id=&#34;our-approach&#34;&gt;Our Approach&lt;/h2&gt;
&lt;p&gt;The first step in generating ROIs is mapping the radar detections from the vehicle coordinates to the camera-view coordinates. Radar detections are reported in a bird’s eye view perspective as shown in image (a) in the figure below, with the object’s range and azimuth measured in the vehicle’s coordinate system. By mapping these detections to the camera-view coordinates, we are
able to associate the objects detected by the Radars to those
seen in the images obtained by the camera.&lt;/p&gt;


















&lt;figure id=&#34;figure-generating-anchors-from-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/anchors.png&#34; data-caption=&#34;Generating anchors from radar detections.&#34;&gt;


  &lt;img src=&#34;./images/anchors.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Generating anchors from radar detections.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;anchor-generation&#34;&gt;Anchor Generation&lt;/h3&gt;
&lt;p&gt;Once the Radar detections are mapped to the image coordinates, we have the approximate location of every detected object in the image. These mapped Radar detections, hereafter
called Points of Interest (POI), provide valuable information
about the objects in each image, without any processing on
the image itself. Having this information, a simple approach
for proposing ROIs would be introducing a bounding box centered at every POI. One problem with this approach is that
Radar detections are not always mapped to the center of the
detected objects in every image. Another problem is the fact
that Radars do not provide any information about the size of
the detected objects and proposing a fixed-size bounding box
for objects of different sizes would not be an effective approach.&lt;/p&gt;
&lt;p&gt;We use the idea of anchor bounding boxes from &lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faster
R-CNN&lt;/a&gt; to alleviate the problems mentioned above. For
every POI, we generate several bounding boxes with different
sizes and aspect ratios centered at the POI, as shown in the figure above (b). We use 4 different sizes and 3 different aspect ratios to
generate these anchors.
To account for the fact that the POI is not always mapped
to the center of the object in the image coordinate, we also
generate different translated versions of the anchors. These
translated anchors provide more accurate bounding boxes
when the POI is mapped towards the right, left or the bottom
of the object as shown in figure above. The generated anchors for a radar detection is shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-anchors-generated-from-a-radar-detection&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop.png&#34; data-caption=&#34;Anchors generated from a radar detection&#34;&gt;


  &lt;img src=&#34;./images/prop.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Anchors generated from a radar detection
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;distance-compensation&#34;&gt;Distance Compensation&lt;/h3&gt;
&lt;p&gt;The distance of each object from the vehicle plays an important role in determining its size in the image. Generally, objects’ sizes in an image have an inverse relationship with their
distance from the camera. Radar detections have the range information for every detected object, which is used in this step
to scale all generated anchors. We use the following formula
to determine the scaling factor to use on the anchors:&lt;/p&gt;
&lt;p&gt;$$ S_i = \alpha \dfrac{1}{d_i} + \beta $$&lt;/p&gt;
&lt;p&gt;where $d_i$
is the distance to the $i$th object, and $\alpha$ and $\beta$ are two
parameters used to adjust the scale factor. These parameters
are learned by maximizing the Intersection Over Union (IOU)
between the generated bounding boxes and the ground truth
bounding boxes in each image. The generated proposals for two radar detection points after distance compensation are shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-proposals-after-distance-compensation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_adj.png&#34; data-caption=&#34;Proposals after distance compensation.&#34;&gt;


  &lt;img src=&#34;./images/prop_adj.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Proposals after distance compensation.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Figures below show a sample image and the proposals generated from all radar detections:&lt;/p&gt;


















&lt;figure id=&#34;figure-sample-image-with-ground-truth-and-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_gt.png&#34; data-caption=&#34;Sample image with ground truth and radar detections&#34;&gt;


  &lt;img src=&#34;./images/prop_gt.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample image with ground truth and radar detections
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-generated-proposals-from-all-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_all.png&#34; data-caption=&#34;Generated proposals from all radar detections.&#34;&gt;


  &lt;img src=&#34;./images/prop_all.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Generated proposals from all radar detections.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The evaluation results are provided in our &lt;a href=&#34;https://arxiv.org/pdf/1905.00526.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICIP 2019 conference paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles</title>
      <link>https://mrnabati.github.io/publication/02_radar-camera-sensor-fusion-for-joint-object-detection-and-distance-estimation-in-autonomous-vehicles/</link>
      <pubDate>Sat, 09 May 2020 16:26:42 -0400</pubDate>
      <guid>https://mrnabati.github.io/publication/02_radar-camera-sensor-fusion-for-joint-object-detection-and-distance-estimation-in-autonomous-vehicles/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles</title>
      <link>https://mrnabati.github.io/publication/00_rrpn-adar-region-proposal-network-for-object-detection-in-autonomous-vehicles/</link>
      <pubDate>Thu, 02 May 2019 23:35:00 +0000</pubDate>
      <guid>https://mrnabati.github.io/publication/00_rrpn-adar-region-proposal-network-for-object-detection-in-autonomous-vehicles/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine-Assisted Annotation of Forensic Imagery</title>
      <link>https://mrnabati.github.io/publication/01_machine-assisted-annotation-of-forensic-imagery/</link>
      <pubDate>Wed, 01 May 2019 23:37:00 +0000</pubDate>
      <guid>https://mrnabati.github.io/publication/01_machine-assisted-annotation-of-forensic-imagery/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>EcoCAR Mobility Challenge</title>
      <link>https://mrnabati.github.io/projects/ecocar-mobility-challenge/</link>
      <pubDate>Thu, 22 Nov 2018 23:55:33 +0000</pubDate>
      <guid>https://mrnabati.github.io/projects/ecocar-mobility-challenge/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://avtcseries.org/ecocar-mobility-challenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR Mobility Challenge&lt;/a&gt; is the newest U.S. Department of Energy (DOE) Advanced Vehicle Technology Competition (AVTC) series challenging 12 North American university teams. It is sponsored by DOE, General Motors and Mathworks and managed by Argonne National Laboratory. Teams have four years to redesign a 2019 Chevrolet Blazer and apply advanced propulsion systems, electrification, SAE Level 2 automation, and vehicle connectivity. Teams will use different sensors and wireless communication devices to design a perception system for the vehicle and deploy &lt;a href=&#34;https://en.wikipedia.org/wiki/Vehicle-to-everything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VehicleToX&lt;/a&gt; (V2X) communication to improve overall operation efficiency in the connected urban environment of the future.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sae.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Society of Automotive Engineers&lt;/a&gt; (SAE) provides a taxonomy for six levels of driving automation, ranging from no driving automation (level 0) to full driving automation (level 5). SAE Level 2 automation refers to a vehicle with combined automated functions, like acceleration and steering, but the driver must remain engaged with the driving task and monitor the environment at all times.&lt;/p&gt;


















&lt;figure id=&#34;figure-the-sae-levels-of-automation-source-nhtsahttpswwwnhtsagovtechnology-innovationautomated-vehicles-safety&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/SAE.png&#34; data-caption=&#34;The SAE Levels of Automation (Source: &amp;lt;a href=&amp;#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&amp;#34;&amp;gt;NHTSA&amp;lt;/a&amp;gt;)&#34;&gt;


  &lt;img src=&#34;./images/SAE.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The SAE Levels of Automation (Source: &lt;a href=&#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&#34;&gt;NHTSA&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;cavs-team&#34;&gt;CAVs Team&lt;/h3&gt;
&lt;p&gt;In the Connected and Automated Vehicle Technologies (CAVs) swimlane, teams will implement connected and automated vehicle technology to improve the stock vehicle’s energy efficiency and utility for a Mobility as a Service (MaaS) carsharing application.&lt;/p&gt;
&lt;p&gt;In particular, the CAVs team is tasked with design and implementation of perception and control systems needed for an SAE Level 2 autonomous vehicle. To design the perception system, teams can integrate different sensors such as cameras, Radars and LiDARs into their vehicles. The CAVs team is also tasked with the design and implementation of a V2X system capable of communicating with infrustructures such as smart traffic lights, or other vehicles with a V2X system.&lt;/p&gt;
&lt;h3 id=&#34;perception-system-design&#34;&gt;Perception System Design&lt;/h3&gt;
&lt;p&gt;Designing a perception system for the vehicle requires a lot of simulation to determine the sensor placement on the vehicle. Matlab&amp;rsquo;s &lt;a href=&#34;https://www.mathworks.com/products/automated-driving.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Driving System Toolbox&lt;/a&gt; provides algorithms and tools for designing and testing ADAS and autonomous driving systems, including tools for sensor placement and Field of View (FOV) simulation. The following figure shows a simple cuboid simulation in this toolbox using the &lt;a href=&#34;https://www.mathworks.com/help/driving/ref/drivingscenariodesigner-app.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Driving Scenario Designer&lt;/a&gt; app.&lt;/p&gt;


















&lt;figure id=&#34;figure-sample-simulation-from-matlabs-driving-scenario-designer-app&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/cuboids.png&#34; data-caption=&#34;Sample Simulation from Matlab&amp;amp;rsquo;s Driving Scenario Designer App&#34;&gt;


  &lt;img src=&#34;./images/cuboids.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample Simulation from Matlab&amp;rsquo;s Driving Scenario Designer App
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After designing and simulating the sensor placements, the sensors are mounted on a mule vehicle to collect real-world data and verify the simulation results. The following video shows the data collected from a LIDAR, front camera and front Radar mounted on
a Chevrolet Camaro while driving on highway. It also displays the outputs of the diagnostic tools designed to monitor the sensor data for possible sensor failure or any other issues while recording the data.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/la5GRq-SrAg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;Data collection&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://avtcseries.org/ecocar-mobility-challenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR Mobility Challenge Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.flickr.com/photos/doeavtc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AVTC on Flicker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;National Highway Traffic Safety Administration, &lt;a href=&#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Vehicles for Safety&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Society of Automotive Engineers, &lt;a href=&#34;https://www.sae.org/standards/content/j3016_201806/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Enabling CAN on Nvidia Jetson Xavier Developer Kit</title>
      <link>https://mrnabati.github.io/2018/11/enabling-can-on-nvidia-jetson-xavier-developer-kit/</link>
      <pubDate>Fri, 09 Nov 2018 20:28:20 +0000</pubDate>
      <guid>https://mrnabati.github.io/2018/11/enabling-can-on-nvidia-jetson-xavier-developer-kit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Installing NVMe SSD on Nvidia Jetson Xavier Developer Kit</title>
      <link>https://mrnabati.github.io/2018/10/installing-nvme-ssd-on-nvidia-jetson-xavier-developer-kit/</link>
      <pubDate>Sat, 06 Oct 2018 22:23:28 +0000</pubDate>
      <guid>https://mrnabati.github.io/2018/10/installing-nvme-ssd-on-nvidia-jetson-xavier-developer-kit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EcoCAR 3</title>
      <link>https://mrnabati.github.io/projects/ecocar-3/</link>
      <pubDate>Thu, 21 Jun 2018 00:07:38 +0000</pubDate>
      <guid>https://mrnabati.github.io/projects/ecocar-3/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://ecocar3.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR 3&lt;/a&gt; is a U.S. Department of Energy (DOE) Advanced
Vehicle Technology Competition (AVTC) series challenging 16 North American
university teams. It is sponsored by DOE and General Motors and managed by Argonne
National Laboratory. Teams have four years to redesign a 2016 Chevrolet Camaro
and convert it into a hybrid vehicle, while still maintaining the performance
expected from this iconic American muscle car.&lt;/p&gt;
&lt;p&gt;EcoCAR 3 is the first AVTC competition requiring teams to implement an Advanced
Driver Assistance System (ADAS) on vehicles. In this competition, ADAS systems
are not allowed to actively intervene in the vehicle’s propulsion or vehicle
control systems and are only allowed to provide passive feedback for drivers.&lt;/p&gt;
&lt;h3 id=&#34;adas-architecture&#34;&gt;ADAS Architecture&lt;/h3&gt;


















&lt;figure id=&#34;figure-utk-adas-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/architecture.png&#34; data-caption=&#34;UTK ADAS Architecture&#34;&gt;


  &lt;img src=&#34;./images/architecture.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    UTK ADAS Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;UT&amp;rsquo;s ADAS architecture is shown in Figure 1. One camera and one front Radar has been
used as the sensor set for this architecture and haptic devices have been installed
in the driver seat to relay the driver feedback signals. Two embedded devices
are used as the main processors in this architecture: an &lt;a href=&#34;https://www.nxp.com/products/processors-and-microcontrollers/arm-based-processors-and-mcus/s32-automotive-platform/s32v-vision-and-sensor-fusion-evaluation-board:S32V234EVB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NXP S32V234 Evaluation Board&lt;/a&gt;
and an &lt;a href=&#34;https://developer.nvidia.com/embedded/buy/jetson-tx2-devkit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nvidia Jetson TX2 Developer Kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Team Tennessee has used the &lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Single Shot MultiBox Detector&lt;/a&gt;
(SSD) as the object detection algorithm and has developed its own sensor fusion algorithm to fuse the
vision based object detection outputs and Radar detections. A sample otuput of the
sensor fusion algorithm is shown in Figure 2. The bounding box displays the
detected object category, an object ID and also the relative distance of the object.&lt;/p&gt;


















&lt;figure id=&#34;figure-fusion-algorithm-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/output1.png&#34; data-caption=&#34;Fusion Algorithm Output&#34;&gt;


  &lt;img src=&#34;./images/output1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Fusion Algorithm Output
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;driver-feedback&#34;&gt;Driver Feedback&lt;/h3&gt;
&lt;p&gt;Driver feedback signals are generated based on the distance of the objects in the same lane as the vehicle,
the relative speed of the objects and the stopping distance of the vehicle.
According to these parameters, three different levels of vibration can be generated
in the haptic devices installed in the driver seat to relay an immediate warning.
The driver feedback system operates as a collision avoidance system, generating
warning signals only when there is a chance of collision.&lt;/p&gt;
&lt;h3 id=&#34;competition-results&#34;&gt;Competition Results&lt;/h3&gt;
&lt;p&gt;In May 2018, Team Tennessee won the award for &lt;strong&gt;Best ADAS Vehicle Demonstration&lt;/strong&gt; at
the EcoCAR 3 Competition held in Yuma AZ, Pomona CA and Los Angeles CA, ranking first
among 16 North American universities in this event. The University of Tennessee
EcoCAR3 team placed sixth overall and won numerous top prizes in this competition.&lt;/p&gt;
&lt;hr&gt;


















&lt;figure id=&#34;figure-team-tennessee-camaro&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/camaro2.png&#34; data-caption=&#34;Team Tennessee Camaro&#34;&gt;


  &lt;img src=&#34;./images/camaro2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Team Tennessee Camaro
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://avtcseries.org/competitions/ecocar3-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR3 webpage&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Spacenet 3: Road Network Detection</title>
      <link>https://mrnabati.github.io/projects/spacenet/</link>
      <pubDate>Fri, 12 Jan 2018 19:47:25 -0400</pubDate>
      <guid>https://mrnabati.github.io/projects/spacenet/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In this challenge, we were tasked with finding automated methods for extracting map-ready road networks from high-resolution satellite imagery. Moving towards more accurate fully automated extraction of road networks will help bring innovation to computer vision methodologies applied to high-resolution satellite imagery, and ultimately help create better maps where they are needed most. The goal is to extract navigable road networks that represent roads from satellite images.&lt;/p&gt;
&lt;p&gt;While Pixel-level F1 score is a widely used segmentation evaluation metric, it is suboptimal for routing applications like road network detection. Because the F1 metric weights each pixel equally, a perfect score is only possible if all pixels are classified correctly as either road or background. With this metric, a brief break in an inferred road (caused for example by an overhanging tree) is lightly penalized while a slight error in road width is penalized heavily. This problem is illustrated in this figure:&lt;/p&gt;


















&lt;figure id=&#34;figure-left-ground-truth-road-mask-in-green-middle-proposal-mask-in-orange-right-proposal-mask-in-cyan-credit-adam-van-ettenhttpsmediumcomthe-downlinqspacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;images/F1_score_issue.png&#34; data-caption=&#34;Left: Ground truth road mask in green. Middle: proposal mask in orange, Right: proposal mask in cyan. Credit: &amp;lt;a href=&amp;#34;https://medium.com/the-downlinq/spacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&amp;#34;&amp;gt;Adam Van Etten&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img src=&#34;images/F1_score_issue.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Left: Ground truth road mask in green. Middle: proposal mask in orange, Right: proposal mask in cyan. Credit: &lt;a href=&#34;https://medium.com/the-downlinq/spacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&#34;&gt;Adam Van Etten&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;According to the figure, while the orange road proposal achieves a lower F1 score (0.82) compared to the cyan proposal (0.95), it is a better proposal because the cyan road mask misses an important intersection and severs a road. This clearly shows that the pixel-based F1 score is suboptimal in this application.&lt;/p&gt;
&lt;p&gt;The Spacenet 3 challenge proposes a graph theoretic metric based upon Dijkstra’s shortest path algorithm, called the Average Path Length Similarity (APLS) metric. APLS sums the differences in optimal path lengths between nodes in the ground truth graph G and the proposal graph G’. More details on this metric is provided &lt;a href=&#34;https://medium.com/the-downlinq/spacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;our-solution&#34;&gt;Our Solution&lt;/h2&gt;
&lt;p&gt;We approached the road detection problem in SpacenNet challenge as a semantic segmentation task in computer vision. Our model is based on a variant of &lt;a href=&#34;https://arxiv.org/abs/1505.04597&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;U-Net&lt;/a&gt;, one of the most successful and popular convolutional neural network architectures for image segmentation. Since we use a image segmentation network to attack this problem, our results are in the form of segmentation masks, which needs to be converted to a line-string graph format. To achieve this, we first extract a binary mask of the detected road network. The result would be something like this:&lt;/p&gt;


















&lt;figure id=&#34;figure-binary-road-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;images/Vegas.png&#34; data-caption=&#34;Binary road map.&#34;&gt;


  &lt;img data-src=&#34;images/Vegas.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Binary road map.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Then we skeletonize the segmentation masks to make it as thin as possible. This step helps with converting the mask to nodes and edges in the next step.&lt;/p&gt;


















&lt;figure id=&#34;figure-skeletonized-road-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;images/Vegas_thin.png&#34; data-caption=&#34;Skeletonized road map.&#34;&gt;


  &lt;img data-src=&#34;images/Vegas_thin.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Skeletonized road map.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Having lines representing detected roads in the image, the next step is converting the lines to line strings. We simply traverse the lines in each continuous segments and keep adding coordinates to a line string. To fill small gaps in the lines, we introduced memory to our traversing algorithm where the algorithm continues in the previous direction for a certain number of steps even after reaching the end of a line. If another line is found, the two line strings are joined and the traversal continues. Depending on the interval used to record the coordinates while traversing, the resulting line strings can have different number of nodes.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
