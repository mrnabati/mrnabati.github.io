<!doctype html><html lang=en><head><meta name=generator content="Hugo 0.109.0"><title>Ramin's Homepage</title><meta name=description content="Portfolio and personal blog of Ramin."><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href=/google-fonts/Mulish/mulish.css><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png><meta property="og:title" content="Ramin's Blog"><meta property="og:type" content="website"><meta property="og:description" content="Portfolio and personal blog of Ramin Nabati."><meta property="og:image" content="/images/author/ramin.png"><meta property="og:url" content="https://raminnabati.com/"><link rel=stylesheet href=/css/sections/home.css><link rel=stylesheet href=/css/sections/about.css><link rel=stylesheet href=/css/sections/skills.css><link rel=stylesheet href=/css/sections/experiences.css><link rel=stylesheet href=/css/sections/education.css><link rel=stylesheet href=/css/sections/projects.css><link rel=stylesheet href=/css/sections/recent-posts.css><link rel=stylesheet href=/css/sections/achievements.css><link rel=stylesheet href=/css/sections/accomplishments.css><link rel=stylesheet href=/css/sections/publications.css><link rel=stylesheet href=/css/style.css><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-129495160-1","auto"),ga("send","pageview"))</script></head><body data-spy=scroll data-target=#top-navbar data-offset=100><nav class="navbar navbar-expand-xl top-navbar initial-navbar" id=top-navbar><div class=container><a class=navbar-brand href=/><img src=/images/site/inverted-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png id=logo alt=Logo>
Ramin's Homepage</a>
<button class="navbar-toggler navbar-dark" id=navbar-toggler type=button data-toggle=collapse data-target=#top-nav-items aria-label=menu>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=#home>Home</a></li><li class=nav-item><a class=nav-link href=#about>About</a></li><li class=nav-item><a class=nav-link href=#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=#education>Education</a></li><li class=nav-item><a class=nav-link href=#projects>Projects</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=#publications>Publications</a>
<a class=dropdown-item href=#recent-posts>Recent Posts</a></div></li><div class=dropdown-divider id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts/>Posts</a></li><li class=nav-item><a class=nav-link id=note-link href=/notes/>Notes</a></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><div class="container-fluid home" id=home><style>#homePageBackgroundImageDivStyled{background-image:url(/images/site/mache_bg_hu4dc3d5064fad82d99424aa1133139c55_3132163_500x0_resize_q75_box.jpg)}@media(min-width:500px) and (max-width:800px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/mache_bg_hu4dc3d5064fad82d99424aa1133139c55_3132163_800x0_resize_q75_box.jpg)}}@media(min-width:801px) and (max-width:1200px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/mache_bg_hu4dc3d5064fad82d99424aa1133139c55_3132163_1200x0_resize_q75_box.jpg)}}@media(min-width:1201px) and (max-width:1500px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/mache_bg_hu4dc3d5064fad82d99424aa1133139c55_3132163_1500x0_resize_q75_box.jpg)}}@media(min-width:1501px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/mache_bg.jpg)}}</style><span class=on-the-fly-behavior></span><div id=homePageBackgroundImageDivStyled class="background container-fluid"></div><div class="container content text-center"><img src=/images/author/ramin_hu577c375f84fd7495871e597685ecc2ca_406533_148x148_fit_box_3.png class="rounded-circle mx-auto d-block img-fluid" alt="Author Image"><h1 class=greeting>Hi, I am Ramin</h1><div class=typing-carousel><span id=ityped class=ityped></span>
<span class=ityped-cursor></span></div><ul id=typing-carousel-data><li>I am an Electrical Engineer</li><li>I am also a Computer Engineer!</li><li>I work on perception for autonomous driving</li></ul><a href=#about class=arrow-center aria-label="Read More - Ramin"><i class="arrow bounce fa fa-chevron-down"></i></a></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container anchor p-lg-5 about-section" id=about><div class="row pt-sm-2 pt-md-4 align-self-center"><div class=col-sm-12><h3 class=p-1>Ramin Nabati</h3><h5 class=p-1>Autonomous Vehicle Sensor Fusion Engineer
at <a href=https://www.corporate.ford.com title="Ford Motor Company" target=_blank rel=noopener>Ford Motor Company</a></h5><p class="p-1 text-justify">I&rsquo;m Ramin, currently working with the Autonomy perception team at Ford, developing perception solutions for autonomous driving. Prior to joining Ford, I was a Ph.D. student at the <a href=https://www.utk.edu/ target=_blank rel=noopener>University of Tennessee Knoxville (UTK)</a> in the <a href=https://aicip.github.io/ target=_blank rel=noopener>Advanced Imaging and Collaborative Information Processing (AICIP)</a> lab advised by <a href=https://www.eecs.utk.edu/people/hairong-qi/ target=_blank rel=noopener>Dr. Hairong Qi</a>, where my research was focused on radar-camera sensor fusion for object detection and tracking in autonomous vehicles. During my time at UTK, I also led the Advanced Driver Assistance Systems (ADAS) and Connected and Automated Vehicles (CAV) teams in the <a href=https://avtcseries.org/about-avtc/past-competitions/ecocar-3/ target=_blank rel=noopener>EcoCAR 3</a> and <a href=https://avtcseries.org/about-avtc/past-competitions/ecocar-mobility-challenge/ target=_blank rel=noopener>EcoCAR Mobility Challenge</a> competitions.</p><div class="text-container ml-auto"><ul class="social-link d-flex"><li><a href=https://www.github.com/mrnabati title=Github target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/ramin-nabati/ title=LinkedIn target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="col-sm-6 pt-5 pl-md-4 pl-sm-3 pt-sm-0"><div class=row></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 experiences-section"><h1 class=text-center><span id=experiences></span>Experiences</h1><div class="container timeline text-justify"><div class="row align-items-center d-flex"><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">1</div></div><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Autonomous Vehicle Sensor Fusion Engineer</h5><h6><a href=https://www.corporate.ford.com title="Ford Motor Company" target=_blank rel=noopener>Ford Motor Company</a></h6><p class=text-muted>Aug 2021 - Present,
Ann Arbor, MI</p></div><p>Ford Autonomous Vehicles (AV) & Mobility is dedicated to enhancing freedom of movement through a suite of mobility products and services, from micromobility to microtransit.</p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Research, design, develop and deploy perception algorithms for autonomous vehicles, including object detection, object tracking and sensor fusion algorithms.</li></ul></div></div><div class="row horizontal-line"><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div><div class="col-10 col-lg-8"><hr></div><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div></div><div class="row align-items-center justify-content-end d-flex"><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Sensor Fusion Intern</h5><h6><a href=https://www.linkedin.com/company/blueberry-technology-inc/ title="Blueberry Technology" target=_blank rel=noopener>Blueberry Technology</a></h6><p class=text-muted>June 2020 - Aug 2020,
San Jose, CA</p></div><p>Blueberry Technology enhances user mobility by providing autonomous wheelchairs for indoor use.</p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Implementing and optimizing a real-time monocular depth estimation algorithm for an autonomous indoor wheelchair. Applying sensor fusion and optimizing the model for the target hardware architecture to achieve real-time performance.</li></ul></div><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">2</div></div></div><div class="row horizontal-line"><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div><div class="col-10 col-lg-8"><hr></div><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div></div><div class="row align-items-center d-flex"><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">3</div></div><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Connected and Automated Vehicle (CAV) Team Lead and GRA</h5><h6><a href=https://avtcseries.org/about-avtc/past-competitions/ecocar-mobility-challenge/ title="EcoCAR Mobility Challenge - UTK" target=_blank rel=noopener>EcoCAR Mobility Challenge - UTK</a></h6><p class=text-muted>August 2018 - Aug 2021,
Knoxville, TN</p></div><p>The EcoCAR Mobility Challenge was the 12th U.S. Department of Energy (DOE) Advanced Vehicle Technology Competition (AVTC) series. The four-year competition challenged 11 university teams to apply advanced propulsion systems, as well as connected and automated vehicle technology to improve the energy efficiency, safety and consumer appeal of the 2019 Chevrolet Blazer.</p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Leading the UTK Connected and Automated Vehicle technologies (CAVs) team in the EcoCAR Mobility Challenge competition. With a focus on SAE level 2 autonomy, the CAVs team designs the onboard perception system, Vehicle-to-X communication system and lateral/longitudinal autonomy system for a 2019 Chevrolet Blazer.</li></ul></div></div><div class="row horizontal-line"><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div><div class="col-10 col-lg-8"><hr></div><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div></div><div class="row align-items-center justify-content-end d-flex"><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Advanced Driver Assistance Systems (ADAS) Team Lead and GRA</h5><h6><a href=https://avtcseries.org/about-avtc/past-competitions/ecocar-3/ title="EcoCAR 3 - UTK" target=_blank rel=noopener>EcoCAR 3 - UTK</a></h6><p class=text-muted>August 2017 - May 2018,
Knoxville, TN</p></div><p>EcoCAR 3 was the 11th U.S. Department of Energy (DOE) Advanced Vehicle Technology Competition (AVTC) series and is North America&rsquo;s premier collegiate automotive engineering competition. The U.S. DOE and General Motors are challenged 16 North American universities to redesign a Chevrolet Camaro into a hybrid-electric car to reduce environmental impact, while maintaining the muscle and performance expected from this iconic American car.</p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Leading the UTK Advanced Driver Assistance System (ADAS) team in the EcoCAR3 competition. The ADAS team focuses on integrating sensing system on a 2016 Chevrolet Camaro and using this system to deploy driver feedback to improve efficiency and safety.</li></ul></div><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">4</div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container-fluid anchor pb-5 education-section"><h1 class=text-center><span id=education></span>Education</h1><div class=container><table class=education-info-table><tbody><tr><td class=icon><div class=hline></div><div class=icon-holder><i class="fas fa-university"></i></div></td><td class=line><div></div></td><td class=details><div class="degree-info card"><div class=row><div class="col-lg-10 col-md-8"><h5><a href=https://utk.edu title="University of Tennessee Knoxville" target=_blank rel=noopener>University of Tennessee Knoxville</a></h5></div><div class="timeframe col-lg-2 col-md-4">2021</div></div><h6>Ph.D in Computer Engineering - Computer Vision</h6><div class=publications><h6 class=text-muted>Publications</h6><ul><li><a href=https://arxiv.org/abs/2011.04841 title="CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection" target=_blank rel=noopener>CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</a></li><li><a href=https://arxiv.org/abs/2009.08428 title="Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles" target=_blank rel=noopener>Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles</a></li><li><a href=https://arxiv.org/abs/1905.00526v2.pdf title="RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles" target=_blank rel=noopener>RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles</a></li></ul></div></div></td></tr><tr><td class=icon><div class=hline></div><div class=icon-holder><i class="fas fa-university"></i></div></td><td class=line><div></div></td><td class=details><div class="degree-info card"><div class=row><div class="col-lg-10 col-md-8"><h5><a href=https://english.iut.ac.ir/ title="Isfahan University of Technology" target=_blank rel=noopener>Isfahan University of Technology</a></h5></div><div class="timeframe col-lg-2 col-md-4">2015</div></div><h6>MSc in Electrical Engineering - Telecommunication Systems</h6></div></td></tr><tr><td class=icon><div class=hline></div><div class=icon-holder><i class="fas fa-university"></i></div></td><td class=line><div></div></td><td class=details><div class="degree-info card"><div class=row><div class="col-lg-10 col-md-8"><h5><a href=# title="Urmia University" target=_blank rel=noopener>Urmia University</a></h5></div><div class="timeframe col-lg-2 col-md-4">2011</div></div><h6>BSc in Electrical Engineering</h6></div></td></tr></tbody></table></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 projects-section" id=projects><h1 class=text-center><span id=projects></span>Projects</h1><div class="container ml-auto text-center"><div class="btn-group flex-wrap" role=group id=project-filter-buttons><button type=button class="btn btn-dark project-filtr-control" data-filter=all>
All</button>
<button type=button class="btn btn-dark project-filtr-control" data-filter=professional>
Professional</button>
<button type=button class="btn btn-dark project-filtr-control" data-filter=academic>
Academic</button>
<button type=button class="btn btn-dark project-filtr-control" data-filter=hobby>
Hobby</button></div></div><div class="container filtr-projects"><div class=row id=project-card-holder><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, Autonomous Driving,Object Detection,Sensor Fusion'><div class="card mt-1"><div class=card><a class=card-header href=posts/projects/06_centerfusion target=_blank rel=noopener><div><div class=d-flex><h5 class="card-title mb-0">CenterFusion</h5></div><div class=sub-title><span>Lead Author</span>
<span>2021</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>A center-based radar and camera fusion for 3D object detection in autonomous vehicles.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">Autonomous Driving</span>
<span class="badge btn-info">Object Detection</span>
<span class="badge btn-info">Sensor Fusion</span></div><div class=project-btn-holder><a class="github-button-inactive project-btn" href=https://github.com/mrnabati/CenterFusion data-icon=octicon-standard data-show-count=true aria-label="Star CenterFusion">Star</a></div></div></div></div></div></div><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, Autonomous Driving,ADAS'><div class="card mt-1"><div class=card><a class=card-header href=posts/projects/02_ecocar3 target=_blank rel=noopener><div><div class=d-flex><h5 class="card-title mb-0">EcoCAR Mobility Challenge</h5></div><div class=sub-title><span>CAV Team Lead</span>
<span>Aug 2018 - Aug 2021</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>12th U.S. Department of Energy (DOE) Advanced Vehicle Technology Competition (AVTC) series, challenging 11 university teams to apply advanced propulsion systems, as well as connected and automated vehicle technology to improve the energy efficiency, safety and consumer appeal of the 2019 Chevrolet Blazer.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">Autonomous Driving</span>
<span class="badge btn-info">ADAS</span></div><div class=project-btn-holder><span><a class="btn btn-outline-info btn-sm" href=posts/projects/02_ecocar3 target=#>Details</a></span></div></div></div></div></div></div><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, Autonomous Driving,ADAS'><div class="card mt-1"><div class=card><a class=card-header href=posts/projects/02_ecocar3 target=_blank rel=noopener><div><div class=d-flex><h5 class="card-title mb-0">EcoCAR 3</h5></div><div class=sub-title><span>ADAS Team Lead</span>
<span>Aug 2017 - May 2018</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>EcoCAR3 is challenging 16 North American university teams to redesign a 2016 Chevrolet Camaro. The ADAS team focuses on integrating the sensing system on the Camaro and deploy driver feedback to improve efficiency and safety.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">Autonomous Driving</span>
<span class="badge btn-info">ADAS</span></div><div class=project-btn-holder><span><a class="btn btn-outline-info btn-sm" href=posts/projects/02_ecocar3 target=#>Details</a></span></div></div></div></div></div></div><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, Segmentation'><div class="card mt-1"><div class=card><a class=card-header href=posts/projects/00_fmow target=_blank rel=noopener><div><div class=d-flex><img class=card-img-xs src=/images/sections/projects/fmow_hu9f418afc80e4b535f795d3de29a7b5ee_149130_24x24_fit_q75_box.jpeg alt="IARPA fMoW Challenge"><h5 class="card-title mb-0">IARPA fMoW Challenge</h5></div><div class=sub-title><span>Contributor</span>
<span>2017</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>The IARPA Functional Map of the World (fMoW) challenge focuses on promoting research in object identification and classification to automatically identify facility, building, and land use from satellite imagery. The dataset consists of 4- and 8-band multispectral images in 63 categories.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">Segmentation</span></div><div class=project-btn-holder><span><a class="btn btn-outline-info btn-sm" href=posts/projects/00_fmow target=#>Details</a></span></div></div></div></div></div></div><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, Segmentation'><div class="card mt-1"><div class=card><a class=card-header href=posts/projects/01_spacenet target=_blank rel=noopener><div><div class=d-flex><img class=card-img-xs src=/images/sections/projects/spacenet_hu76a59520ab2b924ae39ec12ff17ff495_1043449_24x24_fit_box_3.png alt="Spacenet 3: Road Network Detection"><h5 class="card-title mb-0">Spacenet 3: Road Network Detection</h5></div><div class=sub-title><span>Contributor</span>
<span>2018</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>The Spacenet 3 challenge is focused on determining road networks and routing information directly from satellite imagery. The SpaceNet 3 Dataset contains ~8,000 km of roads across the four SpaceNet Areas of Interest.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">Segmentation</span></div><div class=project-btn-holder><a class="github-button-inactive project-btn" href=https://github.com/AICIP-UTK/spacenet-round3 data-icon=octicon-standard data-show-count=true aria-label="Star Spacenet 3: Road Network Detection">Star</a></div></div></div></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container-fluid anchor pb-5 publications-section" id=publications><h1 class=text-center><span id=publications></span>Publications</h1><div class="container ml-auto text-center"><div class="btn-group flex-wrap" role=pub-group id=publication-filter-buttons></div></div><div class="container filtr-publications"><div class=row id=publication-card-holder><div class="col-12 p-2 pub-filtr-item" data-category=pub-all><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</h5><div class=sub-title><span><a href=https://wacv2021.thecvf.com/home>IEEE Winter conference on Applications of Computer Vision (WACV) 2021</a></span>
<span class=ml-2>2021</span></div><div class=authors><span class=mr-2><a href=https://raminnabati.com>Ramin Nabati</a></span>
<span class=mr-2><a href=https://aicip.github.io/>Dr. Hairong Qi</a></span></div></div><div class=card-body><p>In this paper, we focus on the problem of radar and camera sensor fusion and propose a middle-fusion approach to exploit both radar and camera data for 3D object detection. Our approach, called CenterFusion, first uses a center point detection network to detect objects by identifying their center points on the image. It then solves the key data association problem using a novel frustum-based method to associate the radar detections to their corresponding object&rsquo;s center point. The associated radar detections are used to generate radar-based feature maps to complement the image features, and regress to object properties such as depth, rotation and velocity.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Autonomous Driving</span>
<span class="btn badge btn-info ml-1 p-2">Sensor Fusion</span>
<span class="btn badge btn-info ml-1 p-2">Object Detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://arxiv.org/abs/2011.04841 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking</h5><div class=sub-title><span><a href=https://2021.ieee-iv.org/>IEEE Intelligent Vehicles Symposium Workshops (IV Workshops) 2021</a></span>
<span class=ml-2>2021</span></div><div class=authors><span class=mr-2><a href=https://raminnabati.com>Ramin Nabati</a></span>
<span class=mr-2><a href="https://github.com/lharri73?tab=repositories">Landon Harris</a></span>
<span class=mr-2><a href=https://aicip.github.io/>Dr. Hairong Qi</a></span></div></div><div class=card-body><p>In this work, we propose an end-to-end network for joint object detection and tracking based on radar and camera sensor fusion. Our proposed method uses a center-based radar-camera fusion algorithm for object detection and utilizes a greedy algorithm for object association.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Autonomous Driving</span>
<span class="btn badge btn-info ml-1 p-2">Sensor Fusion</span>
<span class="btn badge btn-info ml-1 p-2">Object Tracking</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://arxiv.org/abs/2107.05150 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles</h5><div class=sub-title><span><a href=https://project.inria.fr/ppniv20/>12th Workshop on Planning, Perception and Navigation for Intelligent Vehicles, IROS 2020</a></span>
<span class=ml-2>2020</span></div><div class=authors><span class=mr-2><a href=https://raminnabati.com>Ramin Nabati</a></span>
<span class=mr-2><a href=https://aicip.github.io/>Dr. Hairong Qi</a></span></div></div><div class=card-body><p>In this paper we present a novel radar-camera sensor fusion framework for accurate object detection and distance estimation in autonomous driving scenarios. The proposed architecture uses a middle-fusion approach to fuse the radar point clouds and RGB images.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Autonomous Driving</span>
<span class="btn badge btn-info ml-1 p-2">Sensor Fusion</span>
<span class="btn badge btn-info ml-1 p-2">Object Detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://arxiv.org/abs/2009.08428 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles</h5><div class=sub-title><span><a href=https://www.2019.ieeeicip.org/2019.ieeeicip.org/index.html>IEEE International Conference on Image Processing (ICIP) 2019</a></span>
<span class=ml-2>2019</span></div><div class=authors><span class=mr-2><a href=https://raminnabati.com>Ramin Nabati</a></span>
<span class=mr-2><a href=https://aicip.github.io/>Dr. Hairong Qi</a></span></div></div><div class=card-body><p>In this paper we introduce RRPN, a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for each mapped Radar detection point. These anchor boxes are then transformed and scaled based on the object&rsquo;s distance from the vehicle, to provide more accurate proposals for the detected objects.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Autonomous Driving</span>
<span class="btn badge btn-info ml-1 p-2">Sensor Fusion</span>
<span class="btn badge btn-info ml-1 p-2">Object Detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://arxiv.org/abs/1905.00526v2 target=_blank rel=noopener role=button>Details</a></div></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 recent-posts-section"><h1 class=text-center><span id=recent-posts></span>Recent Posts</h1><div class=container><div class=row id=recent-post-cards><div class="col-lg-4 col-md-6 pt-2 post-card"><a href=/posts/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/ title="Adv. PyTorch: Configuring MS Visual Studio for Using PyToch C++ API in Windows" class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/images/featured.png alt="Card image cap"></div><div class=card-body><h5 class=card-title>Adv. PyTorch: Configuring MS Visual Studio for Using PyToch C++ API in Windows</h5><p class="card-text post-summary">This tutorial will walk you through the required steps to configure and use the PyTorch C++ API (LibTorch) in Microsoft Visual Studio. Although the recommended build system for LibTorch is CMake, you might find yourself in situations where you need to integrate your code into an existing Visual Studio Project/Solution and don&rsquo;t want to deal with CMake files in Windows. Following the steps in this tutorial should get you up and running with LibTorch in Visual Studio without needing to use CMake to build it.</p></div><div class=card-footer><span class=float-left>June 22, 2020</span>
<a href=/posts/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/ title=Read class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class="col-lg-4 col-md-6 pt-2 post-card"><a href=/posts/003_adv_pytorch_modifying_the_last_layer/ title="Adv. PyTorch: Modifying the Last Layer" class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/003_adv_pytorch_modifying_the_last_layer/images/featured.png alt="Card image cap"></div><div class=card-body><h5 class=card-title>Adv. PyTorch: Modifying the Last Layer</h5><p class="card-text post-summary">All the pre-trained models provided in the torchvision package in PyTorch are trained on the ImageNet dataset and can be used out of the box on this dataset. But often times you want to use these models on other available image datasets or even your own custom dataset. This usually requires modifying and fine-tuning the model to work with the new dataset. Changing the output dimension of the last layer in the model is usually among the first changes you need to make, and that&rsquo;s the focus of this post.</p></div><div class=card-footer><span class=float-left>June 21, 2020</span>
<a href=/posts/003_adv_pytorch_modifying_the_last_layer/ title=Read class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class="col-lg-4 col-md-6 pt-2 post-card"><a href=/posts/002_adv_pytorch_freezing_layers/ title="Adv. PyTorch: Freezing Layers" class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/002_adv_pytorch_freezing_layers/images/featured.jpg alt="Card image cap"></div><div class=card-body><h5 class=card-title>Adv. PyTorch: Freezing Layers</h5><p class="card-text post-summary">If you&rsquo;re planning to fine-tune a trained model on a different dataset, chances are you&rsquo;re going to freeze some of the early layers and only update the later layers. I won&rsquo;t go into the details of why you may want to freeze some layers and which ones should be frozen, but I&rsquo;ll show you how to do it in PyTorch. Let&rsquo;s get started!
We first need a pre-trained model to start with.</p></div><div class=card-footer><span class=float-left>May 22, 2020</span>
<a href=/posts/002_adv_pytorch_freezing_layers/ title=Read class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div></div></div></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#publications>Publications</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=https://github.com/mrnabati target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>mrnabati</span></a></li><li><a href=https://www.linkedin.com/in/ramin-nabati target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Ramin Nabati</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">Â© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=/js/itype.min.js></script>
<script src=/js/github-button.js></script>
<script src=/js/home.js></script>
<script src=/js/jquery.filterizr.min.js></script></body></html>