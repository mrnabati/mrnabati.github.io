<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Autonomous Driving | Ramin&#39;s Homepage</title>
    <link>https://mrnabati.github.io/tag/autonomous-driving/</link>
      <atom:link href="https://mrnabati.github.io/tag/autonomous-driving/index.xml" rel="self" type="application/rss+xml" />
    <description>Autonomous Driving</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2020 Ramin Nabati</copyright><lastBuildDate>Sun, 10 May 2020 17:25:44 -0400</lastBuildDate>
    <image>
      <url>https://mrnabati.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Autonomous Driving</title>
      <link>https://mrnabati.github.io/tag/autonomous-driving/</link>
    </image>
    
    <item>
      <title>Radar Region Proposal Network</title>
      <link>https://mrnabati.github.io/projects/rrpn/</link>
      <pubDate>Sun, 10 May 2020 17:25:44 -0400</pubDate>
      <guid>https://mrnabati.github.io/projects/rrpn/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Radar Region Proposal Network (RRPN) is a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object
proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for
each mapped Radar detection point. These anchor boxes are
then transformed and scaled based on the object’s distance
from the vehicle, to provide more accurate proposals for the
detected objects. The generated proposals can be used in any two-stage object detection network
such as Fast-RCNN. Relying only on Radar detections to generate object proposals makes an extremely fast RPN, making
it suitable for autonomous driving applications. Aside from
being a RPN for an object detection algorithm, the proposed
network also inherently acts as a sensor fusion algorithm by
fusing the Radar and camera data to obtain higher accuracy
and reliability.&lt;/p&gt;
&lt;p&gt;RRPN also provides an attention mechanism to focus the
underlying computational resources on the more important
parts of the input data. While in other object detection applications the entire image may be of equal importance. In
an autonomous driving application more attention needs to be
given to objects on the road. For example in a highway driving scenario, the perception system needs to be able to detect
all the vehicles on the road, but there is no need to dedicate resources to detect a picture of a vehicle on a billboard. A Radar
based RPN focuses only on the physical objects surrounding
the vehicle, hence inherently creating an attention mechanism
focusing on parts of the input image that are more important.&lt;/p&gt;
&lt;h2 id=&#34;our-approach&#34;&gt;Our Approach&lt;/h2&gt;
&lt;p&gt;The first step in generating ROIs is mapping the radar detections from the vehicle coordinates to the camera-view coordinates. Radar detections are reported in a bird’s eye view perspective as shown in image (a) in the figure below, with the object’s range and azimuth measured in the vehicle’s coordinate system. By mapping these detections to the camera-view coordinates, we are
able to associate the objects detected by the Radars to those
seen in the images obtained by the camera.&lt;/p&gt;


















&lt;figure id=&#34;figure-generating-anchors-from-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/anchors.png&#34; data-caption=&#34;Generating anchors from radar detections.&#34;&gt;


  &lt;img src=&#34;./images/anchors.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Generating anchors from radar detections.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;anchor-generation&#34;&gt;Anchor Generation&lt;/h3&gt;
&lt;p&gt;Once the Radar detections are mapped to the image coordinates, we have the approximate location of every detected object in the image. These mapped Radar detections, hereafter
called Points of Interest (POI), provide valuable information
about the objects in each image, without any processing on
the image itself. Having this information, a simple approach
for proposing ROIs would be introducing a bounding box centered at every POI. One problem with this approach is that
Radar detections are not always mapped to the center of the
detected objects in every image. Another problem is the fact
that Radars do not provide any information about the size of
the detected objects and proposing a fixed-size bounding box
for objects of different sizes would not be an effective approach.&lt;/p&gt;
&lt;p&gt;We use the idea of anchor bounding boxes from &lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faster
R-CNN&lt;/a&gt; to alleviate the problems mentioned above. For
every POI, we generate several bounding boxes with different
sizes and aspect ratios centered at the POI, as shown in the figure above (b). We use 4 different sizes and 3 different aspect ratios to
generate these anchors.
To account for the fact that the POI is not always mapped
to the center of the object in the image coordinate, we also
generate different translated versions of the anchors. These
translated anchors provide more accurate bounding boxes
when the POI is mapped towards the right, left or the bottom
of the object as shown in figure above. The generated anchors for a radar detection is shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-anchors-generated-from-a-radar-detection&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop.png&#34; data-caption=&#34;Anchors generated from a radar detection&#34;&gt;


  &lt;img src=&#34;./images/prop.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Anchors generated from a radar detection
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;distance-compensation&#34;&gt;Distance Compensation&lt;/h3&gt;
&lt;p&gt;The distance of each object from the vehicle plays an important role in determining its size in the image. Generally, objects’ sizes in an image have an inverse relationship with their
distance from the camera. Radar detections have the range information for every detected object, which is used in this step
to scale all generated anchors. We use the following formula
to determine the scaling factor to use on the anchors:&lt;/p&gt;
&lt;p&gt;$$ S_i = \alpha \dfrac{1}{d_i} + \beta $$&lt;/p&gt;
&lt;p&gt;where $d_i$
is the distance to the $i$th object, and $\alpha$ and $\beta$ are two
parameters used to adjust the scale factor. These parameters
are learned by maximizing the Intersection Over Union (IOU)
between the generated bounding boxes and the ground truth
bounding boxes in each image. The generated proposals for two radar detection points after distance compensation are shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-proposals-after-distance-compensation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_adj.png&#34; data-caption=&#34;Proposals after distance compensation.&#34;&gt;


  &lt;img src=&#34;./images/prop_adj.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Proposals after distance compensation.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Figures below show a sample image and the proposals generated from all radar detections:&lt;/p&gt;


















&lt;figure id=&#34;figure-sample-image-with-ground-truth-and-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_gt.png&#34; data-caption=&#34;Sample image with ground truth and radar detections&#34;&gt;


  &lt;img src=&#34;./images/prop_gt.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample image with ground truth and radar detections
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-generated-proposals-from-all-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_all.png&#34; data-caption=&#34;Generated proposals from all radar detections.&#34;&gt;


  &lt;img src=&#34;./images/prop_all.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Generated proposals from all radar detections.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The evaluation results are provided in our &lt;a href=&#34;https://arxiv.org/pdf/1905.00526.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICIP 2019 conference paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EcoCAR Mobility Challenge</title>
      <link>https://mrnabati.github.io/projects/ecocar-mobility-challenge/</link>
      <pubDate>Thu, 22 Nov 2018 23:55:33 +0000</pubDate>
      <guid>https://mrnabati.github.io/projects/ecocar-mobility-challenge/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://avtcseries.org/ecocar-mobility-challenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR Mobility Challenge&lt;/a&gt; is the newest U.S. Department of Energy (DOE) Advanced Vehicle Technology Competition (AVTC) series challenging 12 North American university teams. It is sponsored by DOE, General Motors and Mathworks and managed by Argonne National Laboratory. Teams have four years to redesign a 2019 Chevrolet Blazer and apply advanced propulsion systems, electrification, SAE Level 2 automation, and vehicle connectivity. Teams will use different sensors and wireless communication devices to design a perception system for the vehicle and deploy &lt;a href=&#34;https://en.wikipedia.org/wiki/Vehicle-to-everything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VehicleToX&lt;/a&gt; (V2X) communication to improve overall operation efficiency in the connected urban environment of the future.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sae.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Society of Automotive Engineers&lt;/a&gt; (SAE) provides a taxonomy for six levels of driving automation, ranging from no driving automation (level 0) to full driving automation (level 5). SAE Level 2 automation refers to a vehicle with combined automated functions, like acceleration and steering, but the driver must remain engaged with the driving task and monitor the environment at all times.&lt;/p&gt;


















&lt;figure id=&#34;figure-the-sae-levels-of-automation-source-nhtsahttpswwwnhtsagovtechnology-innovationautomated-vehicles-safety&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/SAE.png&#34; data-caption=&#34;The SAE Levels of Automation (Source: &amp;lt;a href=&amp;#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&amp;#34;&amp;gt;NHTSA&amp;lt;/a&amp;gt;)&#34;&gt;


  &lt;img src=&#34;./images/SAE.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The SAE Levels of Automation (Source: &lt;a href=&#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&#34;&gt;NHTSA&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;cavs-team&#34;&gt;CAVs Team&lt;/h3&gt;
&lt;p&gt;In the Connected and Automated Vehicle Technologies (CAVs) swimlane, teams will implement connected and automated vehicle technology to improve the stock vehicle’s energy efficiency and utility for a Mobility as a Service (MaaS) carsharing application.&lt;/p&gt;
&lt;p&gt;In particular, the CAVs team is tasked with design and implementation of perception and control systems needed for an SAE Level 2 autonomous vehicle. To design the perception system, teams can integrate different sensors such as cameras, Radars and LiDARs into their vehicles. The CAVs team is also tasked with the design and implementation of a V2X system capable of communicating with infrustructures such as smart traffic lights, or other vehicles with a V2X system.&lt;/p&gt;
&lt;h3 id=&#34;perception-system-design&#34;&gt;Perception System Design&lt;/h3&gt;
&lt;p&gt;Designing a perception system for the vehicle requires a lot of simulation to determine the sensor placement on the vehicle. Matlab&amp;rsquo;s &lt;a href=&#34;https://www.mathworks.com/products/automated-driving.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Driving System Toolbox&lt;/a&gt; provides algorithms and tools for designing and testing ADAS and autonomous driving systems, including tools for sensor placement and Field of View (FOV) simulation. The following figure shows a simple cuboid simulation in this toolbox using the &lt;a href=&#34;https://www.mathworks.com/help/driving/ref/drivingscenariodesigner-app.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Driving Scenario Designer&lt;/a&gt; app.&lt;/p&gt;


















&lt;figure id=&#34;figure-sample-simulation-from-matlabs-driving-scenario-designer-app&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/cuboids.png&#34; data-caption=&#34;Sample Simulation from Matlab&amp;amp;rsquo;s Driving Scenario Designer App&#34;&gt;


  &lt;img src=&#34;./images/cuboids.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample Simulation from Matlab&amp;rsquo;s Driving Scenario Designer App
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After designing and simulating the sensor placements, the sensors are mounted on a mule vehicle to collect real-world data and verify the simulation results. The following video shows the data collected from a LIDAR, front camera and front Radar mounted on
a Chevrolet Camaro while driving on highway. It also displays the outputs of the diagnostic tools designed to monitor the sensor data for possible sensor failure or any other issues while recording the data.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/la5GRq-SrAg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;Data collection&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://avtcseries.org/ecocar-mobility-challenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR Mobility Challenge Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.flickr.com/photos/doeavtc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AVTC on Flicker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;National Highway Traffic Safety Administration, &lt;a href=&#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Vehicles for Safety&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Society of Automotive Engineers, &lt;a href=&#34;https://www.sae.org/standards/content/j3016_201806/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>EcoCAR 3</title>
      <link>https://mrnabati.github.io/projects/ecocar-3/</link>
      <pubDate>Thu, 21 Jun 2018 00:07:38 +0000</pubDate>
      <guid>https://mrnabati.github.io/projects/ecocar-3/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://ecocar3.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR 3&lt;/a&gt; is a U.S. Department of Energy (DOE) Advanced
Vehicle Technology Competition (AVTC) series challenging 16 North American
university teams. It is sponsored by DOE and General Motors and managed by Argonne
National Laboratory. Teams have four years to redesign a 2016 Chevrolet Camaro
and convert it into a hybrid vehicle, while still maintaining the performance
expected from this iconic American muscle car.&lt;/p&gt;
&lt;p&gt;EcoCAR 3 is the first AVTC competition requiring teams to implement an Advanced
Driver Assistance System (ADAS) on vehicles. In this competition, ADAS systems
are not allowed to actively intervene in the vehicle’s propulsion or vehicle
control systems and are only allowed to provide passive feedback for drivers.&lt;/p&gt;
&lt;h3 id=&#34;adas-architecture&#34;&gt;ADAS Architecture&lt;/h3&gt;


















&lt;figure id=&#34;figure-utk-adas-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/architecture.png&#34; data-caption=&#34;UTK ADAS Architecture&#34;&gt;


  &lt;img src=&#34;./images/architecture.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    UTK ADAS Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;UT&amp;rsquo;s ADAS architecture is shown in Figure 1. One camera and one front Radar has been
used as the sensor set for this architecture and haptic devices have been installed
in the driver seat to relay the driver feedback signals. Two embedded devices
are used as the main processors in this architecture: an &lt;a href=&#34;https://www.nxp.com/products/processors-and-microcontrollers/arm-based-processors-and-mcus/s32-automotive-platform/s32v-vision-and-sensor-fusion-evaluation-board:S32V234EVB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NXP S32V234 Evaluation Board&lt;/a&gt;
and an &lt;a href=&#34;https://developer.nvidia.com/embedded/buy/jetson-tx2-devkit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nvidia Jetson TX2 Developer Kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Team Tennessee has used the &lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Single Shot MultiBox Detector&lt;/a&gt;
(SSD) as the object detection algorithm and has developed its own sensor fusion algorithm to fuse the
vision based object detection outputs and Radar detections. A sample otuput of the
sensor fusion algorithm is shown in Figure 2. The bounding box displays the
detected object category, an object ID and also the relative distance of the object.&lt;/p&gt;


















&lt;figure id=&#34;figure-fusion-algorithm-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/output1.png&#34; data-caption=&#34;Fusion Algorithm Output&#34;&gt;


  &lt;img src=&#34;./images/output1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Fusion Algorithm Output
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;driver-feedback&#34;&gt;Driver Feedback&lt;/h3&gt;
&lt;p&gt;Driver feedback signals are generated based on the distance of the objects in the same lane as the vehicle,
the relative speed of the objects and the stopping distance of the vehicle.
According to these parameters, three different levels of vibration can be generated
in the haptic devices installed in the driver seat to relay an immediate warning.
The driver feedback system operates as a collision avoidance system, generating
warning signals only when there is a chance of collision.&lt;/p&gt;
&lt;h3 id=&#34;competition-results&#34;&gt;Competition Results&lt;/h3&gt;
&lt;p&gt;In May 2018, Team Tennessee won the award for &lt;strong&gt;Best ADAS Vehicle Demonstration&lt;/strong&gt; at
the EcoCAR 3 Competition held in Yuma AZ, Pomona CA and Los Angeles CA, ranking first
among 16 North American universities in this event. The University of Tennessee
EcoCAR3 team placed sixth overall and won numerous top prizes in this competition.&lt;/p&gt;
&lt;hr&gt;


















&lt;figure id=&#34;figure-team-tennessee-camaro&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/camaro2.png&#34; data-caption=&#34;Team Tennessee Camaro&#34;&gt;


  &lt;img src=&#34;./images/camaro2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Team Tennessee Camaro
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://avtcseries.org/competitions/ecocar3-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR3 webpage&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
