<!doctype html><html><head><title>Radar-Camera Sensor Fusion and Depth Estimation</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href=/google-fonts/Mulish/mulish.css><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png><meta property="og:title" content="Radar-Camera Sensor Fusion and Depth Estimation"><meta property="og:description" content="A novel radar-camera sensor fusion framework for accurate object detection and distance estimation in autonomous vehicles."><meta property="og:type" content="article"><meta property="og:url" content="https://mrnabati.github.io/posts/projects/05_radar_camera_fusion/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-10T17:25:44-04:00"><meta property="article:modified_time" content="2020-05-10T17:25:44-04:00"><meta name=description content="Radar-Camera Sensor Fusion and Depth Estimation"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-129495160-1","auto"),ga("send","pageview"))</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png alt=Logo>
Ramin's Homepage</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/projects/>Projects</a><ul class=active><li><a href=/posts/projects/00_fmow/ title=fMoW>fMoW</a></li><li><a href=/posts/projects/01_spacenet/ title=SpaceNet3>SpaceNet3</a></li><li><a href=/posts/projects/02_ecocar3/ title=EcoCAR3>EcoCAR3</a></li><li><a href=/posts/projects/03_ecocarmc/ title="EcoCAR Mobility Challenge">EcoCAR Mobility Challenge</a></li><li><a href=/posts/projects/04_rrpn/ title=RRPN>RRPN</a></li><li><a class=active href=/posts/projects/05_radar_camera_fusion/ title=RCSF>RCSF</a></li><li><a href=/posts/projects/06_centerfusion/ title=CenterFusion>CenterFusion</a></li></ul></li><li><a href=https://medium.com/@ramin.nabati/installing-an-nvme-ssd-drive-on-nvidia-jetson-xavier-37183c948978 title="NVMe SSD on Nvidia Jetson Xavier">NVMe SSD on Nvidia Jetson Xavier</a></li><li><a href=https://medium.com/@ramin.nabati/enabling-can-on-nvidia-jetson-xavier-developer-kit-aaaa3c4d99c9 title="Enable CAN on Nvidia Jetson Xavier">Enable CAN on Nvidia Jetson Xavier</a></li><li><a href=/posts/002_adv_pytorch_freezing_layers/ title="Pytorch: Freezing Layers">Pytorch: Freezing Layers</a></li><li><a href=/posts/003_adv_pytorch_modifying_the_last_layer/ title="Pytorch: Modifying Layers">Pytorch: Modifying Layers</a></li><li><a href=/posts/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/ title="Pytorch: C++ API">Pytorch: C++ API</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/projects/05_radar_camera_fusion/images/featured.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ramin_hu577c375f84fd7495871e597685ecc2ca_406533_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Ramin Nabati</h5><p>Sunday, May 10, 2020</p></div><div class=title><h1>Radar-Camera Sensor Fusion and Depth Estimation</h1></div><div class=taxonomy-terms><ul style=padding-left:0><li class=rounded><a href=/tags/autonomous-driving/ class="btn, btn-sm">Autonomous Driving</a></li><li class=rounded><a href=/tags/sensor-fusion/ class="btn, btn-sm">Sensor Fusion</a></li><li class=rounded><a href=/tags/object-detection/ class="btn, btn-sm">Object Detection</a></li></ul></div><div class=post-content id=post-content><h2 id=introduction>Introduction</h2><p>In this project, we designed and implemented a radar-camera fusion algorithm for joint object detection and distance estimation in
autonomous driving applications. The proposed method is
designed as a two-stage object detection network that fuses
radar point clouds and learned image features to generate
accurate object proposals. For every object proposal, a depth
value is also calculated to estimate the object’s distance from
the vehicle. These proposals are then fed into the second
stage of the detection network for object classification. We
evaluate our network on the <a href=https://www.nuscenes.org/ target=_blank rel=noopener>nuScenes dataset</a>, which
provides synchronized data from multiple radar and camera
sensors on a vehicle. Our experiments show that the proposed
method outperforms other radar-camera fusion methods in
the object detection task and is capable of accurately estimating distance for all detected objects.</p><h2 id=approach>Approach</h2><figure><img src=./images/architecture.jpg><figcaption><h4>Network architecture</h4></figcaption></figure><p>Our proposed sensor fusion network is shown in the figure above.
The network takes radar point clouds and RGB images as
input and generates accurate object proposals for a two-stage
object detection framework. We take a middle-fusion approach for fusing the radar and image data, where outputs of each sensor are processed independently first, and are merged
at a later stage for more processing. More specifically, we
first use the radar detections to generate 3D object proposals,
then map the proposals to the image and use the image
features extracted by a backbone network to improve their
localization. These proposals are then merged with image-based proposals generated in a RPN, and are fed to the
second stage for classification. All generated proposals are
associated with an estimated depth, calculated either directly
from the radar detections, or via a distance regressor layer
in the RPN network.</p><h3 id=proposal-generation>Proposal Generation</h3><p>We treat every radar point as a
stand-alone detection and generate 3D object proposals for
them directly without any feature extraction. These proposals
are generated using predefined 3D anchors for every object
class in the dataset. Each 3D anchor is parameterized as
$(x, y, z, w, l, h, r)$, where $(x, y, z)$ is the center, $(w, l, h)$ is
the size, and $r$ is the orientation of the box in vehicle’s
coordinate system. The anchor size, $(w, l, h)$, is fixed for
each object category, and is set to the average size of the
objects in each category in the training dataset. For every radar point, we generate $2n$
boxes from the 3D anchors, where $n$ is the number of object
classes in the dataset, each having two different orientations at $0 $ and $90$ degrees. The 3D anchors for a radar detection is shown in the figure below:</p><figure><img src=./images/3d_prop.jpg><figcaption><h4>3D anchors for one radar detection point</h4></figcaption></figure><p>In the next step, all 3D anchors are mapped to the image
plane and converted to equivalent 2D bounding boxes by
finding the smallest enclosing box for each mapped anchor. Since every 3D proposal is generated from a radar detection,
it has an accurate distance associated with it. This distance is
used as the proposed distance for the generated 2D bounding
box. This is illustrated in the figure below:</p><figure><img src=./images/2d_prop.jpg><figcaption><h4>3D anchors for one radar detection point</h4></figcaption></figure><p>All generated 2D proposals are fed into the
Radar Proposal Refinement (RPR) subnetwork. This is where
the information obtained from the radars (radar proposals) is
fused with the information obtained from the camera (image
features). RPR uses the features extracted from the image
by the backbone network to adjust the size and location
of the radar proposals on the image. As radar detections
are not always centered on the corresponding objects on
the image, the generated 3D anchors and corresponding 2D
proposals might be offset as well. The box regressor layer in
the RPR uses the image features inside each radar proposal
to regress offset values for the proposal corner points. The
RPR also contains a box classification layer, which estimates
an objectness score for every radar proposal. The objectness
score is used to eliminate proposals that are generated by
radar detections coming from background objects, such as
buildings and light poles. Figures below show the resulting 2D radar
proposals before and after the refinement step.</p><figure><img src=./images/2d_all_before.jpg><figcaption><h4>Radar proposals before refinement</h4></figcaption></figure><figure><img src=./images/2d_all_after.jpg><figcaption><h4>Radar proposals after refinement</h4></figcaption></figure><p>The Radar proposals are then merged with image-based proposals obtained from a Region Proposal Network (RPN). Before using these proposals in the next
stage, redundant proposals are removed by applying NonMaximum Suppression (NMS). The NMS would normally
remove overlapping proposals without discriminating based
on the bounding box’s origin, but we note that radar-based
proposals have more reliable distance information than the
image-based proposals. This is because image-based distances are estimated only from 2D image feature maps with
no depth information. To make sure the radar-based distances
are not unnecessarily discarded in the NMS process, we
first calculate the Intersection over Union (IoU) between
radar and image proposals. Next we use an IoU threshold
to find the matching proposals, and overwrite the imagebased distances by their radar-based counterparts for these
matching proposals.</p><h3 id=detection-network>Detection Network</h3><p>The inputs to the second stage detection network are
the feature map from the image and object proposals. The
structure of this network is similar to <a href=https://arxiv.org/pdf/1504.08083.pdf target=_blank rel=noopener>Fast R-CNN</a>. The
feature map is cropped for every object proposals and is fed
into the RoI pooling layer to obtain feature vectors of the
same size for all proposals. These feature vectors are further
processed by a set of fully connected layers and are passed to
the softmax and bounding box regression layers. The output
is the category classification and bounding box regression
for each proposal, in addition to the distance associated to
every detected object. Similar to the RPN network, we use
a cross entropy loss for object classification and a Smooth
L1 loss for the box regression layer.</p><h2 id=results>Results</h2><p>The performance of our method is shown in the table below. This
table shows the overall Average Precision (AP) and Average
Recall (AR) for the detection task, and Mean Absolute
Error for the distance estimation task. We use the <a href=https://arxiv.org/pdf/1506.01497.pdf target=_blank rel=noopener>Faster RCNN</a> network as our image-based detection baseline, and
compare our results with <a href=https://arxiv.org/pdf/1905.00526.pdf target=_blank rel=noopener>RRPN</a>.</p><table><thead><tr><th>Method</th><th>AP</th><th>AP50</th><th>AP75</th><th>AR</th><th>MAE</th></tr></thead><tbody><tr><td>Faster R-CNN</td><td>34.95</td><td>58.23</td><td>36.89</td><td>40.21</td><td>-</td></tr><tr><td>RRPN</td><td>35.45</td><td>59.00</td><td>37.00</td><td>42.10</td><td>-</td></tr><tr><td>Ours</td><td>35.60</td><td>60.53</td><td>37.38</td><td>42.10</td><td>2.65</td></tr></tbody></table><p>The per-class performance is show in the table below:</p><table><thead><tr><th>Method</th><th>Car</th><th>Truck</th><th>Person</th><th>Bus</th><th>Bicycle</th><th>Motorcycle</th></tr></thead><tbody><tr><td>Faster R-CNN</td><td>51.46</td><td>33.26</td><td>27.06</td><td>47.73</td><td>24.27</td><td>25.93</td></tr><tr><td>RRPN</td><td>41.80</td><td>44.70</td><td>17.10</td><td>57.20</td><td>21.40</td><td>30.50</td></tr><tr><td>Ours</td><td>52.31</td><td>34.45</td><td>27.59</td><td>48.30</td><td>25.00</td><td>25.97</td></tr></tbody></table><p>Figures below show the detection results for two different scenes:</p><figure><img src=./images/res_1.jpg></figure><figure><img src=./images/res_2.jpg><figcaption><h4>Detection results</h4></figcaption></figure></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmrnabati.github.io%2fposts%2fprojects%2f05_radar_camera_fusion%2f" target=_blank><i class="fab fa-facebook"></i></a>
<a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fmrnabati.github.io%2fposts%2fprojects%2f05_radar_camera_fusion%2f&text=Radar-Camera%20Sensor%20Fusion%20and%20Depth%20Estimation&via=Ramin%27s%20Homepage" target=_blank><i class="fab fa-twitter"></i></a>
<a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fmrnabati.github.io%2fposts%2fprojects%2f05_radar_camera_fusion%2f&title=Radar-Camera%20Sensor%20Fusion%20and%20Depth%20Estimation" target=_blank><i class="fab fa-reddit"></i></a>
<a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fmrnabati.github.io%2fposts%2fprojects%2f05_radar_camera_fusion%2f&title=Radar-Camera%20Sensor%20Fusion%20and%20Depth%20Estimation" target=_blank><i class="fab fa-linkedin"></i></a>
<a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=Radar-Camera%20Sensor%20Fusion%20and%20Depth%20Estimation https%3a%2f%2fmrnabati.github.io%2fposts%2fprojects%2f05_radar_camera_fusion%2f" target=_blank><i class="fab fa-whatsapp"></i></a>
<a class="btn btn-sm email-btn" href="mailto:?subject=Radar-Camera%20Sensor%20Fusion%20and%20Depth%20Estimation&body=https%3a%2f%2fmrnabati.github.io%2fposts%2fprojects%2f05_radar_camera_fusion%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https:/github.com/mrnabati/mrnabati.github.io/edit/main/content/posts/projects/05_radar_camera_fusion/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/projects/04_rrpn/ title="Radar Region Proposal Network" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Radar Region Proposal Network</div></a></div><div class="col-md-6 next-article"><a href=/posts/projects/06_centerfusion/ title=CenterFusion class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>CenterFusion</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#approach>Approach</a><ul><li><a href=#proposal-generation>Proposal Generation</a></li><li><a href=#detection-network>Detection Network</a></li></ul></li><li><a href=#results>Results</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#publications>Publications</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=https://github.com/mrnabati target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>mrnabati</span></a></li><li><a href=https://www.linkedin.com/in/ramin-nabati target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Ramin Nabati</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script src=/js/mermaid-8.14.0.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script></body></html>