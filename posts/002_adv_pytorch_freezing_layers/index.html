<!doctype html><html><head><title>Adv. PyTorch: Freezing Layers</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href=/google-fonts/Mulish/mulish.css><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png><meta property="og:title" content="Adv. PyTorch: Freezing Layers"><meta property="og:description" content="How to freeze layers of a pre-trained model in PyTorch."><meta property="og:type" content="article"><meta property="og:url" content="https://mrnabati.github.io/posts/002_adv_pytorch_freezing_layers/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-22T13:42:11-04:00"><meta property="article:modified_time" content="2020-05-22T13:42:11-04:00"><meta name=description content="How to freeze layers of a pre-trained model in PyTorch."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-129495160-1","auto"),ga("send","pageview"))</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png alt=Logo>
Ramin's Homepage</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu577c375f84fd7495871e597685ecc2ca_406533_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/projects/>Projects</a><ul><li><a href=/posts/projects/00_fmow/ title=fMoW>fMoW</a></li><li><a href=/posts/projects/01_spacenet/ title=SpaceNet3>SpaceNet3</a></li><li><a href=/posts/projects/02_ecocar3/ title=EcoCAR3>EcoCAR3</a></li><li><a href=/posts/projects/03_ecocarmc/ title="EcoCAR Mobility Challenge">EcoCAR Mobility Challenge</a></li><li><a href=/posts/projects/04_rrpn/ title=RRPN>RRPN</a></li><li><a href=/posts/projects/05_radar_camera_fusion/ title=RCSF>RCSF</a></li><li><a href=/posts/projects/06_centerfusion/ title=CenterFusion>CenterFusion</a></li></ul></li><li><a href=https://medium.com/@ramin.nabati/installing-an-nvme-ssd-drive-on-nvidia-jetson-xavier-37183c948978 title="NVMe SSD on Nvidia Jetson Xavier">NVMe SSD on Nvidia Jetson Xavier</a></li><li><a href=https://medium.com/@ramin.nabati/enabling-can-on-nvidia-jetson-xavier-developer-kit-aaaa3c4d99c9 title="Enable CAN on Nvidia Jetson Xavier">Enable CAN on Nvidia Jetson Xavier</a></li><li><a class=active href=/posts/002_adv_pytorch_freezing_layers/ title="Pytorch: Freezing Layers">Pytorch: Freezing Layers</a></li><li><a href=/posts/003_adv_pytorch_modifying_the_last_layer/ title="Pytorch: Modifying Layers">Pytorch: Modifying Layers</a></li><li><a href=/posts/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/ title="Pytorch: C++ API">Pytorch: C++ API</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/002_adv_pytorch_freezing_layers/images/featured.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ramin_hu577c375f84fd7495871e597685ecc2ca_406533_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Ramin Nabati</h5><p>Friday, May 22, 2020</p></div><div class=title><h1>Adv. PyTorch: Freezing Layers</h1></div><div class=taxonomy-terms><ul style=padding-left:0></ul></div><div class=post-content id=post-content><p>If you&rsquo;re planning to fine-tune a trained model on a different dataset, chances
are you&rsquo;re going to freeze some of the early layers and only update the later
layers. I won&rsquo;t go into the details of why you may want to freeze some layers
and which ones should be frozen, but I&rsquo;ll show you how to do it in PyTorch.
Let&rsquo;s get started!</p><p>We first need a pre-trained model to start with. The
<a href=https://pytorch.org/vision/stable/models.html>models subpackage</a> in
the <code>torchvision</code> package provides definitions for many of the poplular model
architectures for image classification. You can construct these models by simply
calling their constructor, which would initialize the model with random weights.
To use the pre-trained models from the PyTorch Model Zoo, you can call the
constructor with the <code>pretrained=True</code> argument. Let&rsquo;s load the pretrained
VGG16 model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchvision.models <span style=color:#66d9ef>as</span> models
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vgg16 <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>vgg16(pretrained<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>This will start downloading the pretrained model into your computer&rsquo;s PyTorch
cache folder, which usually is the <code>.cache/torch/checkpoints</code> folder under your
home directory.</p><p>There are multiple ways you can look into the model to see its modules and
layers. One way is using the <code>.modules()</code> member function, which returns in
iterator containing all the member objects of the model. The <code>.modules()</code>
functions recursively goes thruogh all the modules and submodules of the model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(list(vgg16<span style=color:#f92672>.</span>modules()))
</span></span></code></pre></div><pre tabindex=0><code>[VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
), Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace=True)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace=True)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace=True)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace=True)
  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (18): ReLU(inplace=True)
  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace=True)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace=True)
  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (25): ReLU(inplace=True)
  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (27): ReLU(inplace=True)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace=True)
  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
), Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AdaptiveAvgPool2d(output_size=(7, 7)), Sequential(
  (0): Linear(in_features=25088, out_features=4096, bias=True)
  (1): ReLU(inplace=True)
  (2): Dropout(p=0.5, inplace=False)
  (3): Linear(in_features=4096, out_features=4096, bias=True)
  (4): ReLU(inplace=True)
  (5): Dropout(p=0.5, inplace=False)
  (6): Linear(in_features=4096, out_features=1000, bias=True)
), Linear(in_features=25088, out_features=4096, bias=True), ReLU(inplace=True), Dropout(p=0.5, inplace=False), Linear(in_features=4096, out_features=4096, bias=True), ReLU(inplace=True), Dropout(p=0.5, inplace=False), Linear(in_features=4096, out_features=1000, bias=True)]
</code></pre><p>That&rsquo;s a lot of information spewed out onto the screen! Let&rsquo;s use the
<code>.named_module()</code> function instead, which returns a (name, module) tuple and
only print the names:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> (name, module) <span style=color:#f92672>in</span> vgg16<span style=color:#f92672>.</span>named_modules():
</span></span><span style=display:flex><span>    print(name)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>features
</span></span><span style=display:flex><span>features.0
</span></span><span style=display:flex><span>features.1
</span></span><span style=display:flex><span>features.2
</span></span><span style=display:flex><span>features.3
</span></span><span style=display:flex><span>features.4
</span></span><span style=display:flex><span>features.5
</span></span><span style=display:flex><span>features.6
</span></span><span style=display:flex><span>features.7
</span></span><span style=display:flex><span>features.8
</span></span><span style=display:flex><span>features.9
</span></span><span style=display:flex><span>features.10
</span></span><span style=display:flex><span>features.11
</span></span><span style=display:flex><span>features.12
</span></span><span style=display:flex><span>features.13
</span></span><span style=display:flex><span>features.14
</span></span><span style=display:flex><span>features.15
</span></span><span style=display:flex><span>features.16
</span></span><span style=display:flex><span>features.17
</span></span><span style=display:flex><span>features.18
</span></span><span style=display:flex><span>features.19
</span></span><span style=display:flex><span>features.20
</span></span><span style=display:flex><span>features.21
</span></span><span style=display:flex><span>features.22
</span></span><span style=display:flex><span>features.23
</span></span><span style=display:flex><span>features.24
</span></span><span style=display:flex><span>features.25
</span></span><span style=display:flex><span>features.26
</span></span><span style=display:flex><span>features.27
</span></span><span style=display:flex><span>features.28
</span></span><span style=display:flex><span>features.29
</span></span><span style=display:flex><span>features.30
</span></span><span style=display:flex><span>avgpool
</span></span><span style=display:flex><span>classifier
</span></span><span style=display:flex><span>classifier.0
</span></span><span style=display:flex><span>classifier.1
</span></span><span style=display:flex><span>classifier.2
</span></span><span style=display:flex><span>classifier.3
</span></span><span style=display:flex><span>classifier.4
</span></span><span style=display:flex><span>classifier.5
</span></span><span style=display:flex><span>classifier.6
</span></span></code></pre></div><p>That&rsquo;s much better! We can see the top level modules are <em>features</em>, <em>avgpool</em>
and <em>classifier</em>. We can also see that the <em>features</em> and <em>calssifier</em> modules
consist of 31 and 7 layers respectively. These layers are not named, and only
have numbers associated with them. If you want to see an even more concise
representation of the network, you can use the <code>.named_children()</code> function
which does not go inside the top level modules recursively:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> (name, module) <span style=color:#f92672>in</span> vgg16<span style=color:#f92672>.</span>named_children():
</span></span><span style=display:flex><span>    print(name)
</span></span></code></pre></div><pre><code>features
avgpool
classifier
</code></pre><p>Now let&rsquo;s see what layers are there under the <em>features</em> module. Here we use the
<code>.children()</code> function to get the layers under the <em>features</em> module, since
these layers are not &rsquo;named':</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> (name, module) <span style=color:#f92672>in</span> vgg16<span style=color:#f92672>.</span>named_children():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;features&#39;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> module<span style=color:#f92672>.</span>children():
</span></span><span style=display:flex><span>            print(layer)
</span></span></code></pre></div><pre tabindex=0><code>Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
</code></pre><p>We can even go deeper and look at the parameters in each layer. Let&rsquo;s get the
parameters of the first layer under the <em>features</em> module:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> (name, module) <span style=color:#f92672>in</span> vgg16<span style=color:#f92672>.</span>named_children():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;features&#39;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> module<span style=color:#f92672>.</span>children():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> layer<span style=color:#f92672>.</span>parameters():
</span></span><span style=display:flex><span>                print(param)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span></code></pre></div><pre tabindex=0><code>Parameter containing:
tensor([[[[-5.5373e-01,  1.4270e-01,  5.2896e-01],
          [-5.8312e-01,  3.5655e-01,  7.6566e-01],
          [-6.9022e-01, -4.8019e-02,  4.8409e-01]],

          [[ 1.7548e-01,  9.8630e-03, -8.1413e-02],
          [ 4.4089e-02, -7.0323e-02, -2.6035e-01],
          [ 1.3239e-01, -1.7279e-01, -1.3226e-01]],

          [[ 3.1303e-01, -1.6591e-01, -4.2752e-01],
          [ 4.7519e-01, -8.2677e-02, -4.8700e-01],
          [ 6.3203e-01,  1.9308e-02, -2.7753e-01]]],


        [[[ 2.3254e-01,  1.2666e-01,  1.8605e-01],
          [-4.2805e-01, -2.4349e-01,  2.4628e-01],
          [-2.5066e-01,  1.4177e-01, -5.4864e-03]],

          [[-1.4076e-01, -2.1903e-01,  1.5041e-01],
          [-8.4127e-01, -3.5176e-01,  5.6398e-01],
          [-2.4194e-01,  5.1928e-01,  5.3915e-01]],

          [[-3.1432e-01, -3.7048e-01, -1.3094e-01],
          [-4.7144e-01, -1.5503e-01,  3.4589e-01],
          [ 5.4384e-02,  5.8683e-01,  4.9580e-01]]],


        [[[ 1.7715e-01,  5.2149e-01,  9.8740e-03],
          [-2.7185e-01, -7.1709e-01,  3.1292e-01],
          [-7.5753e-02, -2.2079e-01,  3.3455e-01]],

          [[ 3.0924e-01,  6.7071e-01,  2.0546e-02],
          [-4.6607e-01, -1.0697e+00,  3.3501e-01],
          [-8.0284e-02, -3.0522e-01,  5.4460e-01]],

          [[ 3.1572e-01,  4.2335e-01, -3.4976e-01],
          [ 8.6354e-02, -4.6457e-01,  1.1803e-02],
          [ 1.0483e-01, -1.4584e-01, -1.5765e-02]]],


        ...,


        [[[ 7.7599e-02,  1.2692e-01,  3.2305e-02],
          [ 2.2131e-01,  2.4681e-01, -4.6637e-02],
          [ 4.6407e-02,  2.8246e-02,  1.7528e-02]],

          [[-1.8327e-01, -6.7425e-02, -7.2120e-03],
          [-4.8855e-02,  7.0427e-03, -1.2883e-01],
          [-6.4601e-02, -6.4566e-02,  4.4235e-02]],

          [[-2.2547e-01, -1.1931e-01, -2.3425e-02],
          [-9.9171e-02, -1.5143e-02,  9.5385e-04],
          [-2.6137e-02,  1.3567e-03,  1.4282e-01]]],


        [[[ 1.6520e-02, -3.2225e-02, -3.8450e-03],
          [-6.8206e-02, -1.9445e-01, -1.4166e-01],
          [-6.9528e-02, -1.8340e-01, -1.7422e-01]],

          [[ 4.2781e-02, -6.7529e-02, -7.0309e-03],
          [ 1.1765e-02, -1.4958e-01, -1.2361e-01],
          [ 1.0205e-02, -1.0393e-01, -1.1742e-01]],

          [[ 1.2661e-01,  8.5046e-02,  1.3066e-01],
          [ 1.7585e-01,  1.1288e-01,  1.1937e-01],
          [ 1.4656e-01,  9.8892e-02,  1.0348e-01]]],


        [[[ 3.2176e-02, -1.0766e-01, -2.6388e-01],
          [ 2.7957e-01, -3.7416e-02, -2.5471e-01],
          [ 3.4872e-01,  3.0041e-02, -5.5898e-02]],

          [[ 2.5063e-01,  1.5543e-01, -1.7432e-01],
          [ 3.9255e-01,  3.2306e-02, -3.5191e-01],
          [ 1.9299e-01, -1.9898e-01, -2.9713e-01]],

          [[ 4.6032e-01,  4.3399e-01,  2.8352e-01],
          [ 1.6341e-01, -5.8165e-02, -1.9196e-01],
          [-1.9521e-01, -4.5630e-01, -4.2732e-01]]]], requires_grad=True)
Parameter containing:
tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,
          0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,
          0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,
          0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,
          0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,
          0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,
          0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,
          0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],
        requires_grad=True)
</code></pre><p>Now that we have access to all the modules, layers and their parameters, we can
easily freeze them by setting the parameters&rsquo; <code>requires_grad</code> flag to <code>False</code>.
This would prevent calculating the gradients for these parameters in the
<code>backward</code> step which in turn prevents the optimizer from updating them.</p><p>Now let&rsquo;s freeze all the parameters in the <em>features</em> module:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>layer_counter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (name, module) <span style=color:#f92672>in</span> vgg16<span style=color:#f92672>.</span>named_children():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;features&#39;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> module<span style=color:#f92672>.</span>children():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> layer<span style=color:#f92672>.</span>parameters():
</span></span><span style=display:flex><span>                param<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#39;Layer &#34;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34; in module &#34;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34; was frozen!&#39;</span><span style=color:#f92672>.</span>format(layer_counter, name))
</span></span><span style=display:flex><span>            layer_counter<span style=color:#f92672>+=</span><span style=color:#ae81ff>1</span>
</span></span></code></pre></div><pre tabindex=0><code>Layer &#34;0&#34; in module &#34;features&#34; was frozen!
Layer &#34;1&#34; in module &#34;features&#34; was frozen!
Layer &#34;2&#34; in module &#34;features&#34; was frozen!
Layer &#34;3&#34; in module &#34;features&#34; was frozen!
Layer &#34;4&#34; in module &#34;features&#34; was frozen!
Layer &#34;5&#34; in module &#34;features&#34; was frozen!
Layer &#34;6&#34; in module &#34;features&#34; was frozen!
Layer &#34;7&#34; in module &#34;features&#34; was frozen!
Layer &#34;8&#34; in module &#34;features&#34; was frozen!
Layer &#34;9&#34; in module &#34;features&#34; was frozen!
Layer &#34;10&#34; in module &#34;features&#34; was frozen!
Layer &#34;11&#34; in module &#34;features&#34; was frozen!
Layer &#34;12&#34; in module &#34;features&#34; was frozen!
Layer &#34;13&#34; in module &#34;features&#34; was frozen!
Layer &#34;14&#34; in module &#34;features&#34; was frozen!
Layer &#34;15&#34; in module &#34;features&#34; was frozen!
Layer &#34;16&#34; in module &#34;features&#34; was frozen!
Layer &#34;17&#34; in module &#34;features&#34; was frozen!
Layer &#34;18&#34; in module &#34;features&#34; was frozen!
Layer &#34;19&#34; in module &#34;features&#34; was frozen!
Layer &#34;20&#34; in module &#34;features&#34; was frozen!
Layer &#34;21&#34; in module &#34;features&#34; was frozen!
Layer &#34;22&#34; in module &#34;features&#34; was frozen!
Layer &#34;23&#34; in module &#34;features&#34; was frozen!
Layer &#34;24&#34; in module &#34;features&#34; was frozen!
Layer &#34;25&#34; in module &#34;features&#34; was frozen!
Layer &#34;26&#34; in module &#34;features&#34; was frozen!
Layer &#34;27&#34; in module &#34;features&#34; was frozen!
Layer &#34;28&#34; in module &#34;features&#34; was frozen!
Layer &#34;29&#34; in module &#34;features&#34; was frozen!
Layer &#34;30&#34; in module &#34;features&#34; was frozen!
</code></pre><p>Now that some of the parameters are frozen, the optimizer needs to be modified
to only get the parameters with <code>requires_grad=True</code>. We can do this by writing
a Lambda function when constructing the optimizer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(filter(<span style=color:#66d9ef>lambda</span> p: p<span style=color:#f92672>.</span>requires_grad, vgg16<span style=color:#f92672>.</span>parameters()), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span></code></pre></div><p>You can now start training your partially frozen model!</p><hr></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmrnabati.github.io%2fposts%2f002_adv_pytorch_freezing_layers%2f" target=_blank><i class="fab fa-facebook"></i></a>
<a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fmrnabati.github.io%2fposts%2f002_adv_pytorch_freezing_layers%2f&text=Adv.%20PyTorch%3a%20Freezing%20Layers&via=Ramin%27s%20Homepage" target=_blank><i class="fab fa-twitter"></i></a>
<a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fmrnabati.github.io%2fposts%2f002_adv_pytorch_freezing_layers%2f&title=Adv.%20PyTorch%3a%20Freezing%20Layers" target=_blank><i class="fab fa-reddit"></i></a>
<a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fmrnabati.github.io%2fposts%2f002_adv_pytorch_freezing_layers%2f&title=Adv.%20PyTorch%3a%20Freezing%20Layers" target=_blank><i class="fab fa-linkedin"></i></a>
<a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=Adv.%20PyTorch%3a%20Freezing%20Layers https%3a%2f%2fmrnabati.github.io%2fposts%2f002_adv_pytorch_freezing_layers%2f" target=_blank><i class="fab fa-whatsapp"></i></a>
<a class="btn btn-sm email-btn" href="mailto:?subject=Adv.%20PyTorch%3a%20Freezing%20Layers&body=https%3a%2f%2fmrnabati.github.io%2fposts%2f002_adv_pytorch_freezing_layers%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https:/github.com/mrnabati/mrnabati.github.io/edit/main/content/posts/002_adv_pytorch_freezing_layers/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/000_enabling_can_on_nvidia_jetson_xavier_developer_kit/ title="Enabling CAN on Nvidia Jetson Xavier" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Enabling CAN on Nvidia Jetson Xavier</div></a></div><div class="col-md-6 next-article"><a href=/posts/003_adv_pytorch_modifying_the_last_layer/ title="Adv. PyTorch: Modifying the Last Layer" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Adv. PyTorch: Modifying the Last Layer</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#publications>Publications</a></li><li class=nav-item><a class=smooth-scroll href=https://mrnabati.github.io/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=https://github.com/mrnabati target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>mrnabati</span></a></li><li><a href=https://www.linkedin.com/in/ramin-nabati target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Ramin Nabati</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">Â© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script src=/js/mermaid-8.14.0.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script></body></html>