<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Ramin&#39;s Homepage</title>
    <link>https://mrnabati.github.io/project/</link>
      <atom:link href="https://mrnabati.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2020 Ramin Nabati</copyright><lastBuildDate>Wed, 21 Nov 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mrnabati.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://mrnabati.github.io/project/</link>
    </image>
    
    <item>
      <title>Radar-Camera Sensor Fusion and Depth Estimation</title>
      <link>https://mrnabati.github.io/project/05_radar_camera_fusion/</link>
      <pubDate>Wed, 14 Oct 2020 21:08:24 -0500</pubDate>
      <guid>https://mrnabati.github.io/project/05_radar_camera_fusion/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this project, we designed and implemented a radar-camera fusion algorithm for joint object detection and distance estimation in
autonomous driving applications. The proposed method is
designed as a two-stage object detection network that fuses
radar point clouds and learned image features to generate
accurate object proposals. For every object proposal, a depth
value is also calculated to estimate the object’s distance from
the vehicle. These proposals are then fed into the second
stage of the detection network for object classification. We
evaluate our network on the &lt;a href=&#34;https://www.nuscenes.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nuScenes dataset&lt;/a&gt;, which
provides synchronized data from multiple radar and camera
sensors on a vehicle. Our experiments show that the proposed
method outperforms other radar-camera fusion methods in
the object detection task and is capable of accurately estimating distance for all detected objects.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;


















&lt;figure id=&#34;figure-network-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/architecture.jpg&#34; data-caption=&#34;Network architecture&#34;&gt;


  &lt;img src=&#34;./images/architecture.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Network architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Our proposed sensor fusion network is shown in the figure above.
The network takes radar point clouds and RGB images as
input and generates accurate object proposals for a two-stage
object detection framework. We take a middle-fusion approach for fusing the radar and image data, where outputs of each sensor are processed independently first, and are merged
at a later stage for more processing. More specifically, we
first use the radar detections to generate 3D object proposals,
then map the proposals to the image and use the image
features extracted by a backbone network to improve their
localization. These proposals are then merged with image-based proposals generated in a RPN, and are fed to the
second stage for classification. All generated proposals are
associated with an estimated depth, calculated either directly
from the radar detections, or via a distance regressor layer
in the RPN network.&lt;/p&gt;
&lt;h3 id=&#34;proposal-generation&#34;&gt;Proposal Generation&lt;/h3&gt;
&lt;p&gt;We treat every radar point as a
stand-alone detection and generate 3D object proposals for
them directly without any feature extraction. These proposals
are generated using predefined 3D anchors for every object
class in the dataset. Each 3D anchor is parameterized as
$(x, y, z, w, l, h, r)$, where $(x, y, z)$ is the center, $(w, l, h)$ is
the size, and $r$ is the orientation of the box in vehicle’s
coordinate system. The anchor size, $(w, l, h)$, is fixed for
each object category, and is set to the average size of the
objects in each category in the training dataset. For every radar point, we generate $2n$
boxes from the 3D anchors, where $n$ is the number of object
classes in the dataset, each having two different orientations at $0 $ and $90$ degrees. The 3D anchors for a radar detection is shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-3d-anchors-for-one-radar-detection-point&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/3d_prop.jpg&#34; data-caption=&#34;3D anchors for one radar detection point&#34;&gt;


  &lt;img src=&#34;./images/3d_prop.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    3D anchors for one radar detection point
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In the next step, all 3D anchors are mapped to the image
plane and converted to equivalent 2D bounding boxes by
finding the smallest enclosing box for each mapped anchor. Since every 3D proposal is generated from a radar detection,
it has an accurate distance associated with it. This distance is
used as the proposed distance for the generated 2D bounding
box. This is illustrated in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-3d-anchors-for-one-radar-detection-point&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/2d_prop.jpg&#34; data-caption=&#34;3D anchors for one radar detection point&#34;&gt;


  &lt;img src=&#34;./images/2d_prop.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    3D anchors for one radar detection point
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;All generated 2D proposals are fed into the
Radar Proposal Refinement (RPR) subnetwork. This is where
the information obtained from the radars (radar proposals) is
fused with the information obtained from the camera (image
features). RPR uses the features extracted from the image
by the backbone network to adjust the size and location
of the radar proposals on the image. As radar detections
are not always centered on the corresponding objects on
the image, the generated 3D anchors and corresponding 2D
proposals might be offset as well. The box regressor layer in
the RPR uses the image features inside each radar proposal
to regress offset values for the proposal corner points. The
RPR also contains a box classification layer, which estimates
an objectness score for every radar proposal. The objectness
score is used to eliminate proposals that are generated by
radar detections coming from background objects, such as
buildings and light poles. Figures below show the resulting 2D radar
proposals before and after the refinement step.&lt;/p&gt;


















&lt;figure id=&#34;figure-radar-proposals-before-refinement&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/2d_all_before.jpg&#34; data-caption=&#34;Radar proposals before refinement&#34;&gt;


  &lt;img src=&#34;./images/2d_all_before.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Radar proposals before refinement
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-radar-proposals-after-refinement&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/2d_all_after.jpg&#34; data-caption=&#34;Radar proposals after refinement&#34;&gt;


  &lt;img src=&#34;./images/2d_all_after.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Radar proposals after refinement
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The Radar proposals are then merged with image-based proposals obtained from a Region Proposal Network (RPN). Before using these proposals in the next
stage, redundant proposals are removed by applying NonMaximum Suppression (NMS). The NMS would normally
remove overlapping proposals without discriminating based
on the bounding box’s origin, but we note that radar-based
proposals have more reliable distance information than the
image-based proposals. This is because image-based distances are estimated only from 2D image feature maps with
no depth information. To make sure the radar-based distances
are not unnecessarily discarded in the NMS process, we
first calculate the Intersection over Union (IoU) between
radar and image proposals. Next we use an IoU threshold
to find the matching proposals, and overwrite the imagebased distances by their radar-based counterparts for these
matching proposals.&lt;/p&gt;
&lt;h3 id=&#34;detection-network&#34;&gt;Detection Network&lt;/h3&gt;
&lt;p&gt;The inputs to the second stage detection network are
the feature map from the image and object proposals. The
structure of this network is similar to &lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast R-CNN&lt;/a&gt;. The
feature map is cropped for every object proposals and is fed
into the RoI pooling layer to obtain feature vectors of the
same size for all proposals. These feature vectors are further
processed by a set of fully connected layers and are passed to
the softmax and bounding box regression layers. The output
is the category classification and bounding box regression
for each proposal, in addition to the distance associated to
every detected object. Similar to the RPN network, we use
a cross entropy loss for object classification and a Smooth
L1 loss for the box regression layer.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The performance of our method is shown in the table below. This
table shows the overall Average Precision (AP) and Average
Recall (AR) for the detection task, and Mean Absolute
Error for the distance estimation task. We use the &lt;a href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faster RCNN&lt;/a&gt; network as our image-based detection baseline, and
compare our results with &lt;a href=&#34;https://arxiv.org/pdf/1905.00526.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RRPN&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;AP&lt;/th&gt;
&lt;th&gt;AP50&lt;/th&gt;
&lt;th&gt;AP75&lt;/th&gt;
&lt;th&gt;AR&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Faster R-CNN&lt;/td&gt;
&lt;td&gt;34.95&lt;/td&gt;
&lt;td&gt;58.23&lt;/td&gt;
&lt;td&gt;36.89&lt;/td&gt;
&lt;td&gt;40.21&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RRPN&lt;/td&gt;
&lt;td&gt;35.45&lt;/td&gt;
&lt;td&gt;59.00&lt;/td&gt;
&lt;td&gt;37.00&lt;/td&gt;
&lt;td&gt;42.10&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ours&lt;/td&gt;
&lt;td&gt;35.60&lt;/td&gt;
&lt;td&gt;60.53&lt;/td&gt;
&lt;td&gt;37.38&lt;/td&gt;
&lt;td&gt;42.10&lt;/td&gt;
&lt;td&gt;2.65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The per-class performance is show in the table below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Car&lt;/th&gt;
&lt;th&gt;Truck&lt;/th&gt;
&lt;th&gt;Person&lt;/th&gt;
&lt;th&gt;Bus&lt;/th&gt;
&lt;th&gt;Bicycle&lt;/th&gt;
&lt;th&gt;Motorcycle&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Faster R-CNN&lt;/td&gt;
&lt;td&gt;51.46&lt;/td&gt;
&lt;td&gt;33.26&lt;/td&gt;
&lt;td&gt;27.06&lt;/td&gt;
&lt;td&gt;47.73&lt;/td&gt;
&lt;td&gt;24.27&lt;/td&gt;
&lt;td&gt;25.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RRPN&lt;/td&gt;
&lt;td&gt;41.80&lt;/td&gt;
&lt;td&gt;44.70&lt;/td&gt;
&lt;td&gt;17.10&lt;/td&gt;
&lt;td&gt;57.20&lt;/td&gt;
&lt;td&gt;21.40&lt;/td&gt;
&lt;td&gt;30.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ours&lt;/td&gt;
&lt;td&gt;52.31&lt;/td&gt;
&lt;td&gt;34.45&lt;/td&gt;
&lt;td&gt;27.59&lt;/td&gt;
&lt;td&gt;48.30&lt;/td&gt;
&lt;td&gt;25.00&lt;/td&gt;
&lt;td&gt;25.97&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Figures below show the detection results for two different scenes:&lt;/p&gt;


















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/res_1.jpg&#34; &gt;


  &lt;img src=&#34;./images/res_1.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;



















&lt;figure id=&#34;figure-detection-results&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/res_2.jpg&#34; data-caption=&#34;Detection results&#34;&gt;


  &lt;img src=&#34;./images/res_2.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Detection results
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Radar Region Proposal Network</title>
      <link>https://mrnabati.github.io/project/04_rrpn/</link>
      <pubDate>Sun, 10 May 2020 17:25:44 -0400</pubDate>
      <guid>https://mrnabati.github.io/project/04_rrpn/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Radar Region Proposal Network (RRPN) is a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object
proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for
each mapped Radar detection point. These anchor boxes are
then transformed and scaled based on the object’s distance
from the vehicle, to provide more accurate proposals for the
detected objects. The generated proposals can be used in any two-stage object detection network
such as Fast-RCNN. Relying only on Radar detections to generate object proposals makes an extremely fast RPN, making
it suitable for autonomous driving applications. Aside from
being a RPN for an object detection algorithm, the proposed
network also inherently acts as a sensor fusion algorithm by
fusing the Radar and camera data to obtain higher accuracy
and reliability.&lt;/p&gt;
&lt;p&gt;RRPN also provides an attention mechanism to focus the
underlying computational resources on the more important
parts of the input data. While in other object detection applications the entire image may be of equal importance. In
an autonomous driving application more attention needs to be
given to objects on the road. For example in a highway driving scenario, the perception system needs to be able to detect
all the vehicles on the road, but there is no need to dedicate resources to detect a picture of a vehicle on a billboard. A Radar
based RPN focuses only on the physical objects surrounding
the vehicle, hence inherently creating an attention mechanism
focusing on parts of the input image that are more important.&lt;/p&gt;
&lt;h2 id=&#34;our-approach&#34;&gt;Our Approach&lt;/h2&gt;
&lt;p&gt;The first step in generating ROIs is mapping the radar detections from the vehicle coordinates to the camera-view coordinates. Radar detections are reported in a bird’s eye view perspective as shown in image (a) in the figure below, with the object’s range and azimuth measured in the vehicle’s coordinate system. By mapping these detections to the camera-view coordinates, we are
able to associate the objects detected by the Radars to those
seen in the images obtained by the camera.&lt;/p&gt;


















&lt;figure id=&#34;figure-generating-anchors-from-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/anchors.png&#34; data-caption=&#34;Generating anchors from radar detections.&#34;&gt;


  &lt;img src=&#34;./images/anchors.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Generating anchors from radar detections.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;anchor-generation&#34;&gt;Anchor Generation&lt;/h3&gt;
&lt;p&gt;Once the Radar detections are mapped to the image coordinates, we have the approximate location of every detected object in the image. These mapped Radar detections, hereafter
called Points of Interest (POI), provide valuable information
about the objects in each image, without any processing on
the image itself. Having this information, a simple approach
for proposing ROIs would be introducing a bounding box centered at every POI. One problem with this approach is that
Radar detections are not always mapped to the center of the
detected objects in every image. Another problem is the fact
that Radars do not provide any information about the size of
the detected objects and proposing a fixed-size bounding box
for objects of different sizes would not be an effective approach.&lt;/p&gt;
&lt;p&gt;We use the idea of anchor bounding boxes from &lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faster
R-CNN&lt;/a&gt; to alleviate the problems mentioned above. For
every POI, we generate several bounding boxes with different
sizes and aspect ratios centered at the POI, as shown in the figure above (b). We use 4 different sizes and 3 different aspect ratios to
generate these anchors.
To account for the fact that the POI is not always mapped
to the center of the object in the image coordinate, we also
generate different translated versions of the anchors. These
translated anchors provide more accurate bounding boxes
when the POI is mapped towards the right, left or the bottom
of the object as shown in figure above. The generated anchors for a radar detection is shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-anchors-generated-from-a-radar-detection&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop.png&#34; data-caption=&#34;Anchors generated from a radar detection&#34;&gt;


  &lt;img src=&#34;./images/prop.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Anchors generated from a radar detection
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;distance-compensation&#34;&gt;Distance Compensation&lt;/h3&gt;
&lt;p&gt;The distance of each object from the vehicle plays an important role in determining its size in the image. Generally, objects’ sizes in an image have an inverse relationship with their
distance from the camera. Radar detections have the range information for every detected object, which is used in this step
to scale all generated anchors. We use the following formula
to determine the scaling factor to use on the anchors:&lt;/p&gt;
&lt;p&gt;$$ S_i = \alpha \dfrac{1}{d_i} + \beta $$&lt;/p&gt;
&lt;p&gt;where $d_i$
is the distance to the $i$th object, and $\alpha$ and $\beta$ are two
parameters used to adjust the scale factor. These parameters
are learned by maximizing the Intersection Over Union (IOU)
between the generated bounding boxes and the ground truth
bounding boxes in each image. The generated proposals for two radar detection points after distance compensation are shown in the figure below:&lt;/p&gt;


















&lt;figure id=&#34;figure-proposals-after-distance-compensation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_adj.png&#34; data-caption=&#34;Proposals after distance compensation.&#34;&gt;


  &lt;img src=&#34;./images/prop_adj.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Proposals after distance compensation.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Figures below show a sample image and the proposals generated from all radar detections:&lt;/p&gt;


















&lt;figure id=&#34;figure-sample-image-with-ground-truth-and-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_gt.png&#34; data-caption=&#34;Sample image with ground truth and radar detections&#34;&gt;


  &lt;img src=&#34;./images/prop_gt.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample image with ground truth and radar detections
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-generated-proposals-from-all-radar-detections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/prop_all.png&#34; data-caption=&#34;Generated proposals from all radar detections.&#34;&gt;


  &lt;img src=&#34;./images/prop_all.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Generated proposals from all radar detections.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The evaluation results are provided in our &lt;a href=&#34;https://arxiv.org/pdf/1905.00526.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICIP 2019 conference paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EcoCAR Mobility Challenge</title>
      <link>https://mrnabati.github.io/project/03_ecocarmc/</link>
      <pubDate>Thu, 22 Nov 2018 23:55:33 +0000</pubDate>
      <guid>https://mrnabati.github.io/project/03_ecocarmc/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://avtcseries.org/ecocar-mobility-challenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR Mobility Challenge&lt;/a&gt; is the newest U.S. Department of Energy (DOE) Advanced Vehicle Technology Competition (AVTC) series challenging 12 North American university teams. It is sponsored by DOE, General Motors and Mathworks and managed by Argonne National Laboratory. Teams have four years to redesign a 2019 Chevrolet Blazer and apply advanced propulsion systems, electrification, SAE Level 2 automation, and vehicle connectivity. Teams will use different sensors and wireless communication devices to design a perception system for the vehicle and deploy &lt;a href=&#34;https://en.wikipedia.org/wiki/Vehicle-to-everything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VehicleToX&lt;/a&gt; (V2X) communication to improve overall operation efficiency in the connected urban environment of the future.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sae.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Society of Automotive Engineers&lt;/a&gt; (SAE) provides a taxonomy for six levels of driving automation, ranging from no driving automation (level 0) to full driving automation (level 5). SAE Level 2 automation refers to a vehicle with combined automated functions, like acceleration and steering, but the driver must remain engaged with the driving task and monitor the environment at all times.&lt;/p&gt;


















&lt;figure id=&#34;figure-the-sae-levels-of-automation-source-nhtsahttpswwwnhtsagovtechnology-innovationautomated-vehicles-safety&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/SAE.png&#34; data-caption=&#34;The SAE Levels of Automation (Source: &amp;lt;a href=&amp;#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&amp;#34;&amp;gt;NHTSA&amp;lt;/a&amp;gt;)&#34;&gt;


  &lt;img src=&#34;./images/SAE.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The SAE Levels of Automation (Source: &lt;a href=&#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&#34;&gt;NHTSA&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;cavs-team&#34;&gt;CAVs Team&lt;/h3&gt;
&lt;p&gt;In the Connected and Automated Vehicle Technologies (CAVs) swimlane, teams will implement connected and automated vehicle technology to improve the stock vehicle’s energy efficiency and utility for a Mobility as a Service (MaaS) carsharing application.&lt;/p&gt;
&lt;p&gt;In particular, the CAVs team is tasked with design and implementation of perception and control systems needed for an SAE Level 2 autonomous vehicle. To design the perception system, teams can integrate different sensors such as cameras, Radars and LiDARs into their vehicles. The CAVs team is also tasked with the design and implementation of a V2X system capable of communicating with infrustructures such as smart traffic lights, or other vehicles with a V2X system.&lt;/p&gt;
&lt;h3 id=&#34;perception-system-design&#34;&gt;Perception System Design&lt;/h3&gt;
&lt;p&gt;Designing a perception system for the vehicle requires a lot of simulation to determine the sensor placement on the vehicle. Matlab&amp;rsquo;s &lt;a href=&#34;https://www.mathworks.com/products/automated-driving.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Driving System Toolbox&lt;/a&gt; provides algorithms and tools for designing and testing ADAS and autonomous driving systems, including tools for sensor placement and Field of View (FOV) simulation. The following figure shows a simple cuboid simulation in this toolbox using the &lt;a href=&#34;https://www.mathworks.com/help/driving/ref/drivingscenariodesigner-app.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Driving Scenario Designer&lt;/a&gt; app.&lt;/p&gt;


















&lt;figure id=&#34;figure-sample-simulation-from-matlabs-driving-scenario-designer-app&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/cuboids.png&#34; data-caption=&#34;Sample Simulation from Matlab&amp;amp;rsquo;s Driving Scenario Designer App&#34;&gt;


  &lt;img src=&#34;./images/cuboids.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample Simulation from Matlab&amp;rsquo;s Driving Scenario Designer App
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After designing and simulating the sensor placements, the sensors are mounted on a mule vehicle to collect real-world data and verify the simulation results. The following video shows the data collected from a LIDAR, front camera and front Radar mounted on
a Chevrolet Camaro while driving on highway. It also displays the outputs of the diagnostic tools designed to monitor the sensor data for possible sensor failure or any other issues while recording the data.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/la5GRq-SrAg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;Data collection&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://avtcseries.org/ecocar-mobility-challenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR Mobility Challenge Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.flickr.com/photos/doeavtc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AVTC on Flicker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;National Highway Traffic Safety Administration, &lt;a href=&#34;https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Vehicles for Safety&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Society of Automotive Engineers, &lt;a href=&#34;https://www.sae.org/standards/content/j3016_201806/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>EcoCAR 3</title>
      <link>https://mrnabati.github.io/project/02_ecocar3/</link>
      <pubDate>Thu, 21 Jun 2018 00:07:38 +0000</pubDate>
      <guid>https://mrnabati.github.io/project/02_ecocar3/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://ecocar3.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR 3&lt;/a&gt; is a U.S. Department of Energy (DOE) Advanced
Vehicle Technology Competition (AVTC) series challenging 16 North American
university teams. It is sponsored by DOE and General Motors and managed by Argonne
National Laboratory. Teams have four years to redesign a 2016 Chevrolet Camaro
and convert it into a hybrid vehicle, while still maintaining the performance
expected from this iconic American muscle car.&lt;/p&gt;
&lt;p&gt;EcoCAR 3 is the first AVTC competition requiring teams to implement an Advanced
Driver Assistance System (ADAS) on vehicles. In this competition, ADAS systems
are not allowed to actively intervene in the vehicle’s propulsion or vehicle
control systems and are only allowed to provide passive feedback for drivers.&lt;/p&gt;
&lt;h3 id=&#34;adas-architecture&#34;&gt;ADAS Architecture&lt;/h3&gt;


















&lt;figure id=&#34;figure-utk-adas-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/architecture.png&#34; data-caption=&#34;UTK ADAS Architecture&#34;&gt;


  &lt;img src=&#34;./images/architecture.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    UTK ADAS Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;UT&amp;rsquo;s ADAS architecture is shown in Figure 1. One camera and one front Radar has been
used as the sensor set for this architecture and haptic devices have been installed
in the driver seat to relay the driver feedback signals. Two embedded devices
are used as the main processors in this architecture: an &lt;a href=&#34;https://www.nxp.com/products/processors-and-microcontrollers/arm-based-processors-and-mcus/s32-automotive-platform/s32v-vision-and-sensor-fusion-evaluation-board:S32V234EVB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NXP S32V234 Evaluation Board&lt;/a&gt;
and an &lt;a href=&#34;https://developer.nvidia.com/embedded/buy/jetson-tx2-devkit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nvidia Jetson TX2 Developer Kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Team Tennessee has used the &lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Single Shot MultiBox Detector&lt;/a&gt;
(SSD) as the object detection algorithm and has developed its own sensor fusion algorithm to fuse the
vision based object detection outputs and Radar detections. A sample otuput of the
sensor fusion algorithm is shown in Figure 2. The bounding box displays the
detected object category, an object ID and also the relative distance of the object.&lt;/p&gt;


















&lt;figure id=&#34;figure-fusion-algorithm-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/output1.png&#34; data-caption=&#34;Fusion Algorithm Output&#34;&gt;


  &lt;img src=&#34;./images/output1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Fusion Algorithm Output
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;driver-feedback&#34;&gt;Driver Feedback&lt;/h3&gt;
&lt;p&gt;Driver feedback signals are generated based on the distance of the objects in the same lane as the vehicle,
the relative speed of the objects and the stopping distance of the vehicle.
According to these parameters, three different levels of vibration can be generated
in the haptic devices installed in the driver seat to relay an immediate warning.
The driver feedback system operates as a collision avoidance system, generating
warning signals only when there is a chance of collision.&lt;/p&gt;
&lt;h3 id=&#34;competition-results&#34;&gt;Competition Results&lt;/h3&gt;
&lt;p&gt;In May 2018, Team Tennessee won the award for &lt;strong&gt;Best ADAS Vehicle Demonstration&lt;/strong&gt; at
the EcoCAR 3 Competition held in Yuma AZ, Pomona CA and Los Angeles CA, ranking first
among 16 North American universities in this event. The University of Tennessee
EcoCAR3 team placed sixth overall and won numerous top prizes in this competition.&lt;/p&gt;
&lt;hr&gt;


















&lt;figure id=&#34;figure-team-tennessee-camaro&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/camaro2.png&#34; data-caption=&#34;Team Tennessee Camaro&#34;&gt;


  &lt;img src=&#34;./images/camaro2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Team Tennessee Camaro
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://avtcseries.org/competitions/ecocar3-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EcoCAR3 webpage&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Spacenet 3: Road Network Detection</title>
      <link>https://mrnabati.github.io/project/01_spacenet/</link>
      <pubDate>Fri, 12 Jan 2018 19:47:25 -0400</pubDate>
      <guid>https://mrnabati.github.io/project/01_spacenet/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In this challenge, we were tasked with finding automated methods for extracting map-ready road networks from high-resolution satellite imagery. Moving towards more accurate fully automated extraction of road networks will help bring innovation to computer vision methodologies applied to high-resolution satellite imagery, and ultimately help create better maps where they are needed most. The goal is to extract navigable road networks that represent roads from satellite images.&lt;/p&gt;
&lt;p&gt;While Pixel-level F1 score is a widely used segmentation evaluation metric, it is suboptimal for routing applications like road network detection. Because the F1 metric weights each pixel equally, a perfect score is only possible if all pixels are classified correctly as either road or background. With this metric, a brief break in an inferred road (caused for example by an overhanging tree) is lightly penalized while a slight error in road width is penalized heavily. This problem is illustrated in this figure:&lt;/p&gt;


















&lt;figure id=&#34;figure-left-ground-truth-road-mask-in-green-middle-proposal-mask-in-orange-right-proposal-mask-in-cyan-credit-adam-van-ettenhttpsmediumcomthe-downlinqspacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;images/F1_score_issue.png&#34; data-caption=&#34;Left: Ground truth road mask in green. Middle: proposal mask in orange, Right: proposal mask in cyan. Credit: &amp;lt;a href=&amp;#34;https://medium.com/the-downlinq/spacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&amp;#34;&amp;gt;Adam Van Etten&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img src=&#34;images/F1_score_issue.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Left: Ground truth road mask in green. Middle: proposal mask in orange, Right: proposal mask in cyan. Credit: &lt;a href=&#34;https://medium.com/the-downlinq/spacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&#34;&gt;Adam Van Etten&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;According to the figure, while the orange road proposal achieves a lower F1 score (0.82) compared to the cyan proposal (0.95), it is a better proposal because the cyan road mask misses an important intersection and severs a road. This clearly shows that the pixel-based F1 score is suboptimal in this application.&lt;/p&gt;
&lt;p&gt;The Spacenet 3 challenge proposes a graph theoretic metric based upon Dijkstra’s shortest path algorithm, called the Average Path Length Similarity (APLS) metric. APLS sums the differences in optimal path lengths between nodes in the ground truth graph G and the proposal graph G’. More details on this metric is provided &lt;a href=&#34;https://medium.com/the-downlinq/spacenet-road-detection-and-routing-challenge-part-i-d4f59d55bfce&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;our-solution&#34;&gt;Our Solution&lt;/h2&gt;
&lt;p&gt;We approached the road detection problem in SpacenNet challenge as a semantic segmentation task in computer vision. Our model is based on a variant of &lt;a href=&#34;https://arxiv.org/abs/1505.04597&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;U-Net&lt;/a&gt;, one of the most successful and popular convolutional neural network architectures for image segmentation. Since we use a image segmentation network to attack this problem, our results are in the form of segmentation masks, which needs to be converted to a line-string graph format. To achieve this, we first extract a binary mask of the detected road network. The result would be something like this:&lt;/p&gt;


















&lt;figure id=&#34;figure-binary-road-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;images/Vegas.png&#34; data-caption=&#34;Binary road map.&#34;&gt;


  &lt;img data-src=&#34;images/Vegas.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Binary road map.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Then we skeletonize the segmentation masks to make it as thin as possible. This step helps with converting the mask to nodes and edges in the next step.&lt;/p&gt;


















&lt;figure id=&#34;figure-skeletonized-road-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;images/Vegas_thin.png&#34; data-caption=&#34;Skeletonized road map.&#34;&gt;


  &lt;img data-src=&#34;images/Vegas_thin.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Skeletonized road map.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Having lines representing detected roads in the image, the next step is converting the lines to line strings. We simply traverse the lines in each continuous segments and keep adding coordinates to a line string. To fill small gaps in the lines, we introduced memory to our traversing algorithm where the algorithm continues in the previous direction for a certain number of steps even after reaching the end of a line. If another line is found, the two line strings are joined and the traversal continues. Depending on the interval used to record the coordinates while traversing, the resulting line strings can have different number of nodes.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
